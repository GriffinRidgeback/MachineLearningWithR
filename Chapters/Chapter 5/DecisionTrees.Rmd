---
title: "Divide and Conquer - Classification using Decision Trees and Rules"
author: "Kevin D'Elia"
date: "May 13th, 2017"
output: 
  html_document: 
    highlight: kate
    number_sections: yes
    theme: spacelab
    toc: yes
  html_notebook: 
    highlight: pygments
    number_sections: yes
    theme: cosmo
    toc: yes
---
# Overview
This notebook covers decision trees and rule learners - two machine learning methods that also make complex decisions from sets of simple choices.  These methods then present their knowledge in the form of logical structures that can be understood with no statistical knowledge.  This aspect makes these models particularly useful for business strategy and process improvement.

# Understanding decision trees
Decision tree learners are powerful classifiers, which utilize a **tree structure** to model the relationships among the features and the potential outcomes.A decision tree classifier uses a structure of branching decisions, which channel examples into a final predicted class value.

Consider a job offer.  The offer itself is the **root node**, and the offer is then passed through **decision nodes** that require choices to be made based on the attributes of the job.  These choices split the data across **branches** that indicate potential outcomes of a decision, depicted as _yes_ or _no_ outcomes, though in some cases there may be more than two possibilities.  In the case where a final decision can be made, the tree is terminated by **leaf nodes**, also known as **terminal nodes** that denote the action to be taken as the result of the series of decisions.  In the case of a predictive model, the leaf nodes provide the expected result given the series of events in the tree.

A great benefit of decision tree algorithms is that the flowchart-like tree structure is not necessarily exclusively for the learner's internal use.  After the model is created, many decision tree algorithms output the resulting structure in a human-readable format.  This provides tremendous insight into how and why the model works or doesn't work well for a particular task.  This also makes decision trees particularly appropriate for applications in which the classification mechanism needs to be transparent for legal reasons, or in case the results need to be shared with others in order to inform future business practices.  Uses include:

* Credit scoring models in which the criteria that causes an applicant to be rejected need to be clearly documented and free from bias
* Marketing studies of customer behavior such as satisfaction or churn, which will be shared with management or advertising agencies
* Diagnosis of medical conditions based on laboratory measurements, symptoms, or the rate of disease progression

Decision trees are perhaps the single most widely used machine learning technique but are not well suited for scenarios where the data has a large number of nominal features with many levels or it has a large number of numeric features.  These cases may result in a very large number of decisions and an overly complex tree.

## Divide and Conquer
Decision trees are built using a heuristic called **recursive partitioning**.  This approach is also commonly known as **divide and conquer** because it splits the data into subsets, which are then split repeatedly into ever smaller subsets, until the algorithm determines the data within the subsets are sufficiently homogeneous, or another stopping criterion has been met.  The initial split criteria is the feature most predictive of the target class. The examples are then partitioned into groups according to the distinct values of this feature.  Splitting continues in this way until a stopping criterion is reached, such as:

* all (or nearly all) of the examples at the node have the same class
* there are no remaining features to distinguish among the examples
* the tree has grown to a predefined size limit

# The C5.0 decision tree algorithm

This is the industry standard to produce decision trees, because it does well for most types of problems directly out of the box.  It can have trouble modeling some relationships due to reliance on **axis-parallel splits**.

## Choosing the best split
The first challenge that a decision tree will face is to identify which feature to split upon, usually resulting in partitions consisting primarily of a single class.  The degree to which a subset of examples contains only a single class is known as **purity**, and any subset composed of only a single class is called **pure**.

There are various measurements of purity that can be used to identify the best decision tree splitting candidate.  C5.0 uses **entropy**, a concept borrowed from information theory that quantifies the randomness, or disorder, within a set of class values.  Sets with high entropy are very diverse and provide little information about other items that may also belong in the set, as there is no apparent commonality.  The decision tree hopes to find splits that reduce entropy, ultimately increasing homogeneity within the groups.

Typically, entropy is measured in **bits**.  If there are only two possible classes, entropy values can range from 0 to 1.  For _n_ classes, entropy ranges from 0 to _log~2~(n)_.  In each case, the minimum value indicates that the sample is completely homogeneous, while the maximum value indicates that the data are as diverse as possible, and no group has even a small plurality.

The mathematical representation for entropy is:

**Entropy(_S_)** = $\sum_{i=1}^{c}$ _-p~i~ log~2~(p~i~)_

In this formula, for a given segment of data _(S)_, the term _c_ refers to the number of class levels and _p~i~_ refers to the proportion of values falling into class level _i_.  For example, given a partition of data with two classes: red (60%) and white (40%), entropy is calculated as follows:
```{r entropy}
-0.60 * log2(0.60) - 0.40 * log2(0.40)
```

This is how to look at entropy for all possible two-class arrangements.  If the proportion of examples in one class is _x_, then the proportion in the other class is _(1 - x)_.  The ```curve()``` function will plot the entropy for all possible values of _x_:
```{r entropy_curve}
x <- 0.60
xx <- 1 - x
curve(-x * log2(x) - xx * log2(xx), col = "red", xlab = "x", ylab = "Entropy", lwd = 4)
```


For _x = 0.50_, a 50-50 split results in maximum entropy; as one class increasingly dominates the other, the entropy reduces to zero.

To use entropy to determine the optimal feature to split upon, the algorithm calculates the change in homogeneity that would result from a split on each possible feature, which is a measure know as **information gain**.  The information gain for a feature _F_ is calculated as the difference between the entropy in the segment before the split _(S~1~)_ and the partitions resulting from the split _(S~2~)_:

**InfoGain(_F_)** = Entropy(_S~1~_) - Entropy(_S~2~_)

One complication is that after a split, the data is divided into more than one partition.  Therefore, the function to calculate _Entropy(S~2~)_ needs to consider the total entropy across all of the partitions.  It does this by weighing each partitions' entropy by the proportion of records falling into the partition.  Formulaically, this is:

**Entropy(_S_)** = $\sum_{i=1}^{n}$ _w~i~_ Entropy(_P~i~_)

In simple terms, the total entropy resulting from a split is the sum of the entropy of each of the _n_ partitions weighted by the proportion of examples falling in the partition (_w~i~_).

The higher the information gain, the better a feature is at creating homogeneous groups after a split on this feature  If the information gain is zero, there is no reduction in entropy for splitting on this feature.  On the other hand, the maximum information gain is equal to the entropy prior to the split.  This would imply that the entropy after the split is zero, which means that the split results in completely homogeneous groups.

The previous formulae assume nominal features, but decision trees use information gain for splitting on numeric features as well.  To do so, a common practice is to test various splits that divide the values into groups greater than or less than a numeric threshold.  This reduces the numeric feature into a two-level categorical feature that allows information gain to be calculated as usual.  The numeric cut point yielding the largest information gain is chosen for the split.

## Pruning the decision tree
A decision tree can continue to grow indefinitely, choosing splitting features and dividing the data into smaller and smaller partitions until each example is perfectly classified or the algorithm runs out of features to split on.  However, if the tree grows overly large, many of the decisions it makes will be overly specific and the model will be overfitted to the training data.  The process of **pruning** a decision tree involves reducing its size such that it generalizes better to unseen data.

One solution to this problem is to stop the tree from growing once it reaches a certain number of decisions or when the decision nodes contain only a small number of examples.  This is called **early stopping** or **pre-pruning** the decision tree.  As the tree avoids doing needless work, this is an appealing strategy.  However, one downside to this approach is that there is no way to know whether the tree will miss subtle, but important patterns that it would have learned had it grown to a larger size.

An alternative, called **post-pruning**, involves growing a tree that is intentionally too large and pruning leaf nodes to reduce the size of the tree to a more appropriate level.  This is often a more effective approach than pre-pruning, because it is quite difficult to determine the optimal depth of a decision tree without growing it first.  Pruning the tree later on allow the algorithm to be certain that all the important data structures were discovered.

C5.0 has an overall strategy of post-pruning.  Nodes and branches that have little effect on the classification errors are removed from the very large tree.  In some cases, entire branches are moved further up the tree or replaced by simpler decisions.  These processes of grafting branches are known as **subtree raising** and **subtree replacement**, respectively.

# Example - identifying risky bank loans using C5.0 decision trees
Decision trees are widely used in the banking industry due to their high accuracy and ability to formulate a statistical model in plain language.  Since government organizations in many countries carefully monitor lending practices, executives must be able to explain why one applicant was rejected for a loan while the others were approved.  This information is also useful for customers hoping to determine why their credit rating is unsatisfactory.

## Step 1 - collecting data
The idea being the credit model is to identify factors that are predictive of higher risk of default.  Therefore, the data needs to contain a large number of past bank loans and whether the loan went into default, as well as information on the applicant.

The credit dataset used includes 1,000 examples on loans, plus a set of numeric and nominal features indicating the characteristics of the loan and the loan applicant.  A class variable indicates whether the loan went into default.

## Step 2 - exploring the data

Since the majority of features are already nominal in nature, the ```stringsAsFactors``` value will be left to its default value of ```TRUE```
```{r load_data}
credit <- read.csv("credit.csv")
str(credit)
```

The _table()_ output shows a couple of loan features that seem likely to predict a default:
```{r table_output}
table(credit$checking_balance)
table(credit$savings_balance)
```

The checking and savings account balance may prove to be important predictors of loan default status.  Some of the loan's features are numeric, such as its duration and the amount of credit requested:
```{r summaries}
summary(credit$months_loan_duration)
summary(credit$amount)
```

The ```default``` vector indicates whether the loan applicant was unable to meet the agreed payment terms and went into default.  A total of 30% of the loans in this dataset went into default:
```{r defaulted_loans}
table(credit$default)
```

A high rate of default is undesirable for a bank, because it means that the bank is unlikely to fully recover its investment.  If the model is successful, it will identify applicants that are at high risk to default, allowing the bank to refuse credit requests.

### Data preparation - creating random training and test datasets
To create the training and test datasets, it is necessary to consider if the data in question is ordered or unordered.  In this case, there is a strong likelihood that the credit data might be sorted by loan amount, so if the training data was 90% and the testing data the remaining 10%, the model will be developed on a biased set of data.  This can by solved by using a **random sample**; reproducibility is achieved by using a **seed** value:
```{r sample_data}
set.seed(123)
train_sample <- sample(1000, 900)
credit_train <- credit[train_sample, ]
credit_test <- credit[-train_sample, ]
```

This code should result in roughly the same results for defaulted loans that was available in the full dataset:
```{r defaults_table}
round(prop.table(table(credit_train$default)), 2)
round(prop.table(table(credit_test$default)), 1)
```

## Step 3 - training a model on the data

**Building the classifier:**

```m <- C5.0(train, class, trials, = 1, costs = NULL)```

* _train_ is a data frame containing training data
* _class_ is a factor vector with the class for each row in the training data
* _trials_ is an optional number to control the number of boosting iterations (set to 1 by default)
* _costs_ is an optional matrix specifying costs associated with various types.

The function will return a C5.0 model object that can be used to make predictions.

**Making predictions:**

```p <- predict(m, test, type = "class")```

* _m_ is a model trained by the C5.0 function
* _test_ is a data frame containing test data with the same features as the training data used to build the classifier
* _type_ is either "class" or "prob" and specifies whether the predictions should be the most probable class or the raw predicted probabilities

The function will return a vector of predicted class values or raw predicted probabilities depending upon the value of the **type** parameter.

Since the _default_ class variable is to be predicted, it must be excluded from the training data frame but supplied as the target factor vector for classification:
```{r credit_model}
library(C50)
credit_model <- C5.0(credit_train[-17], credit_train$default)
credit_model
```

The output shows some simple facts about the tree, including the function call that generated it, the number of features (labeled _predictors_), and examples (labeled _samples_) used to grow the tree.  The tree size indicates the decision depth of the tree.  The _summary()_ function shows the tree's decisions:
```{r tree_summary}
summary(credit_model)
```

The summary output shows some of the first branches in the decision tree.  The first three lines could be represented in plain language as:

1. If the checking account balance is unknown or greater than 200 DM, then classify as "not likely to default."
2. Otherwise, if the checking account balance is lees than 0 DM or between 1 and 200 DM.
3. And the credit history is perfect or very good, then classify as "likely to default."

The numbers in parentheses indicate the number of examples meeting the criteria for that decision, and the number incorrectly classified by the decision.  For instance, on the first line, 412/50 indicates that of the 412 examples reaching the decision, 50 were incorrectly classified as not likely to default.  In other words, 50 applicants actually defaulted in spite of the model's prediction to the contrary.

After the tree, the ```summary(credit_model)``` output displays a confusion matrix, which is a cross-tabulation that indicates the model's incorrectly classified records in the training data.  The _Errors_ output notes that the model correctly classified all but 133 of the 900 training instances for an error rate of 14.8%.  A total of 35 actual _no_ values were incorrectly classified as _ues_ (false positives), while 98 _yes_ values were misclassified as _no_ (false negatives).

## Step 4 - evaluating model performance

Use the _predict()_ function now on the test dataset:
```{r tree_predict}
credit_pred <- predict(credit_model, credit_test)
```

This creates a vector pf predicted class values, which can be compared to the actual class values using the _CrossTable()_ function; the row and column percentages have been suppressed:
```{r predictions}
library(gmodels)
CrossTable(credit_test$default,
           credit_pred,
           prop.chisq = FALSE,
           prop.c = FALSE,
           prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```

## Step 5 - improving model performance

The model's error rate is likely to be too high to deploy it in a real-time credit scoring application.  In fact, if the model had predicted "no default" for every test case, it would have been correct 67% of the time - a result not much worse than the model's but requiring much less effort.  Making matters even worse, the model performed especially poorly at identifying applicants who do default on their loans.

### Boosting the accuracy of decision trees

Using **adaptive boosting**, a process in which many decision trees are built and the trees vote on the best class for each example, the model can be improved.  Boosting is rooted in the notion that by combining a number of weak performing learners, a team that is much stronger than any of the learners alone can be created.  Each of the models has a unique set of strengths and weaknesses and they may be better or worse in solving certain problems.  Using a combination of several learners with complementary strengths and weaknesses can therefore dramatically improve the accuracy of a classifier.

By adding an additional ```trials``` parameter indicating the number of separate decision trees to use in the boosted team and setting an upper limit, the algorithm will stop adding trees if it recognizes that additional trials do not seem to be improving the accuracy.
```{r trials}
credit_boost10 <- C5.0(credit_train[-17],
                       credit_train$default,
                       trials = 10)
```

While examining the resulting model, some additional lines have been added, indicating the changes:
```{r trials_results}
credit_boost10
```

So the tree size has shrunk.  All 10 trees, along with the model's performance, can be summarized:
```{r trials_summary}
summary(credit_boost10)
```

The classifier made 34 mistakes on 900 training examples for an error rate of 3.8%, which is quite an improvement over the 13.9% training error rate noted before adding boosting.  Now see if the improvement are realized in the test data:
```{r trials_test}
credit_boost_pred10 <- predict(credit_boost10, credit_test)
CrossTable(credit_test$default,
           credit_boost_pred10,
           prop.chisq = FALSE,
           prop.c = FALSE,
           prop.r = FALSE,
           dnn = c("actual default", "predicted default"))
```

The total error rate was reduced from 27% prior to boosting down to 18% in the boosted model but the model is still not doing well at predicting defaults, predicting only 20/33 = 61% correctly.  The lack of an even greater improvement may be a function of the relatively small training dataset or it may just be a very difficult problem to solve.

This said, if boosting can be added this easily, why not apply it all the time?  Two reasons:

1. building one decision tree can be computationally expensive and building many trees may be impractical
2. if the training data is very noisy, then boosting might not result in an improvement at all

### Making mistakes more costlier than others

Giving a loan out to an applicant who is likely to default can be an expensive mistake.  One solution to reduce the number of false negatives may be to reject a larger number of borderline applicant, under the assumption that the interest the bank would earn from a risky loan is far outweighed by the massive loss it would incur if the money is not paid back at all.

The C5.0 algorithm allows the assignment of a penalty to different types of errors, in order to discourage a tree from making more costly mistakes.  The penalties are designated in a **cost matrix**, which specifies how much costlier each error is, relative to any other prediction.

To begin constructing the cost matrix, start by specifying the dimensions.  Since the predicted and actual values can both take two values, _yes_ or _no_, a 2x2 matrix is needed, using a list of two vectors, each with two values.  Giving names to the matrix dimensions will avoid later confusion.

```{r cost_matrix}
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimensions) <- c("predicted", "actual")
```

Next, assign the penalty for the various types of errors by supplying four values to fill the matrix, like so:

* Predicted no, actual no
* Predicted yes, actual no
* Predicted no, actual yes
* Predicted yes, actual yes

Suppose the theory is that a loan default costs the bank four times as much as a missed opportunity.  The penalty values would then be defined as:
```{r error_cost}
error_cost <- matrix(c(0, 1, 4, 0), nrow = 2, dimnames = matrix_dimensions)
```

As defined by this matrix, there is no cost assigned when the algorithm classifies a _no_ or _yes_ correctly, but a false negative has a cost of 4 versus a false positive's cost of 1.  Apply this now to the existing data:
```{r cost_predict}
# apply the cost matrix to the tree
credit_cost <- C5.0(credit_train[-17], 
                    credit_train$default,
                    costs = error_cost)

credit_cost_pred <- predict(credit_cost, credit_test)

CrossTable(credit_test$default, 
           credit_cost_pred,
           prop.chisq = FALSE, 
           prop.c = FALSE, 
           prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```

Compared to the boosted model, this version makes more mistakes overall:  37% error versus 18% in the boosted case.  However, the types of mistakes are very different.  There the previous models incorrectly classified only 42 and 61% of defaults correctly, in this model, 79% of the actual defaults were predicted to be non-defaults.  This trade resulting in a reduction of false negatives at the expense of increasing false positives may be acceptable if our cost estimates were accurate.

# Understanding classification rules
Classification rules represent knowledge in the form of logical if-else statements that assign a class to unlabeled examples.  They are specified in terms of an **antecedent** and a **consequent**; these form a hypothesis stating that "if this happens, then that happens.", such as a hard drive clicking means that it will fail soon.  The antecedent comprises certain combinations of feature values, while the consequent specifies the class value to assign when the rule's conditions are met.

Rule learners are often used in a manner similar to decision tree learners.  Like decision trees, they can be used for applications that generate knowledge for future action, such as:

* Identifying conditions that lead to a hardware failure in mechanical devices
* Describing the key characteristics of groups of people for customer segmentation
* Finding conditions that precede large drops or increases in the prices of shares on the stock market

On the other hand, rule learners offer some distinct advantages over trees for some tasks.  Unlike a tree, which must be applied from top-to-bottom through a series of decisions, rules are propositions that can be read much like a statement of fact.  Additionally, the results of a rule learner can be more simple, direct, and easier to understand than a decision tree built on the same data.

Rule learners are generally applied to problems where the features are primarily or entirely nominal.  They do well at identifying rare events, even if the rare event occurs only for a very specific interaction among feature values.

# Separate and conquer
Classification rule learning algorithms utilize a heuristic known as **separate and conquer**.  The process involves identifying a rule that covers a subset of examples in the training data, and then separating this partition from the remaining data.  As the rules are added, additional subsets of the data are separated until the entire dataset has been covered and no more examples remain.

One way to imagine the rule learning process is to think about drilling down into the data by creating increasingly specific rules to identify class values.  To identify whether or not an animal is a mammal, the set of all animals can be depicted as a large space.  A rule learner begins by using the available features to find homogeneous groups.  For example, using a feature that indicates whether the species travels via land, sea, or air. the first rule might suggest that any land-based animals are mammals.  But the rule might include animals like frogs, which are not mammals.  A more specific rule would state that mammals must walk on land and have a tail.  An additional rule might require that to be classified as a mammal, the animal must have fur.  And so on.  This shows how rules gradually consume larger and larger segments of data to eventually classify all instances.

As the rules seem to cover portions of the data, separate and conquer algorithms are also known as **covering algorithms**, and the resulting rules are called **covering rules**.

# The 1R Algorithm

The simplest classifier, **ZeroR**, is a rule learner that literally learns no rules (hence the name).  For every unlabeled example, regardless of the values of its features, it predicts the most common class.

The **1R algorithm** (**One Rule** or **OneR**) improves over ZeroR by selecting a single rule.  The way this algorithm works is simple.  For each feature, 1R divides the data into groups based on similar values of the feature.  Then, for each segment, the algorithm predicts the majority class.  The error rate for the rule based on each feature is calculated and the rule with the fewest errors is chosen as the one rule.  Obviously, this rule learning algorithm may be too basic for some tasks, such as considering only a single symptom in a medical diagnosis system.

# The RIPPER algorithm
Early rule learning algorithms were plagued by a couple of problems.  First, they were notorious for being slow, which made them ineffective for the increasing number of large datasets.  Secondly, they were often prone to being inaccurate on noisy data.

A first step was the **Incremental Reduced Error Pruning (IREP) algorithm** which uses a combination of pre-pruning and post-pruning methods that grow very complex rules and prune them before separating the instances from the full dataset.  Although this strategy helped the performance of rule learners, decision trees often still performed better.

The **Repeated Incremental Pruning to Produce Error Reduction (RIPPER) algorithm** improved upon IREP to generate rules that match or exceed the performance of decision trees.  Having evolved from several iterations of rule learning algorithms, the RIPPER algorithm is a patchwork of efficient heuristics for rule learning.  It can be understood in general terms as a three-step process:

1. Grow
2. Prune
3. Optimize

The growing phase uses the separate and conquer technique to greedily add conditions to a rule until it perfectly classifies a subset of data or runs out of attributes for splitting.  Similar to decision trees, the information gain criterion is used to identify the next splitting attribute.  When increasing a rule's specificity no longer reduces entropy, the rule is immediately pruned.  Steps one and two are repeated until it reaches a stopping criterion, at which point the entire set of rules is optimized using a variety of heuristics.

# Rules from decision trees
Classification rules can also be obtained directly from decision trees.  Beginning at a leaf node and following the branches back to the root results in a series of decisions.  These can be combined into a single rule.  The chief downside to using a decision tree to generate rules is that the resulting rules are often more complex than those learned by a rule learning algorithm.  The divide and conquer strategy employed by decision trees biases the results differently than that of a rule learner.  On the other hand, it is sometimes more computationally efficient to generate rules from trees.  The ```C5.0``` function will generate a model using classification rules if you specify ```rules = TRUE``` when training the model.

# What makes trees and rules greedy?
Decision trees and rule learners are known as **greedy learners** because they use data on a first-come, first-served basis.  Both the divide and conquer heuristic used by decision trees and the separate and conquer heuristic used by rule learners attempt to make partitions one at at time, finding the most homogeneous partition first. followed by the next best, and so on, until all examples have been classified.

The downside to the greedy approach is that greedy algorithms are not guaranteed to generate the optimal, most accurate, or smallest number of rules for a particular dataset.  By taking the low-hanging fruit early, a greedy learner may quickly find a single rule that is accurate for one subset of data; however, in doing so, the learner may miss the opportunity to develop a more nuanced set of rules with better overall accuracy on the entire set of data.  However, without using the greedy approach to rule learning, it is likely that for all but the smallest of datasets, rule learning would be computationally infeasible.

Though both trees and rules employ greedy learning heuristics, there are subtle differences in how they build rules.  Perhaps the best way to distinguish them is to note that once divide and conquer splits on a feature, the partitions created by the split may not be re-conquered, only further subdivided.  In this way, a tree is permanently limited by its history of past decisions.  In contrast, once separate and conquer finds a rule, any examples not covered by all of the rule's conditions may be re-conquered.

# Example - identifying poisonouse mushrooms with rule learners
Unlike the identification of harmful plants such as poison oak or poison ivy, there are no clear rules to identify whether a wild mushroom is poisonous or edible.  If simple, clear, and consistent rules were available to identify poisonous mushrooms, they could save the lives of foragers.

## Step 1 - collecting the data

The dataset used includes information on 8,124 mushroom samples from 23 species of gilled mushrooms and consists of two classes:  poisonous and non-poisonous.

## Step 2 - exploring and preparing the data

Since all the 22 features and the target class are nominal, use ```stringsAsFactors``` equal to **TRUE**.
```{r read_mushrooms}
mushrooms <- read.csv("mushrooms.csv", stringsAsFactors = TRUE)
```

Looking at the output of ```str()```, one feature is worth mentioning: the _veil_type_ variable:
```{r veil_type}
str(mushrooms)
```

It is a factor with one level, however the data dictionary lists two levels for this feature: partial and universal.  Since all the examples in our data are classified as partial, it is likely that this data element was somehow coded incorrectly.  It probably does not provide any useful information for prediction and can safely be removed:
```{r drop_veil}
mushrooms$veil_color <- NULL
```

Looking at the distribution of mushrooms types: 
```{r mushroom_types}
table(mushrooms$type)
```

shows that about 52% of the mushroom samples are edible, while 48% are poisonous. 

For the purposes of this example, consider that the 8,214 samples in the mushroom data to be an exhaustive set of all the possible wild mushrooms.  This is an important assumption, because it means that no samples out of the training data will be pulled for testing purposes.  No effort is made to develop rules that cover unforeseen types of mushrooms; the goal here is to find rules that accurately depict the complete set of known mushroom types.  Therefore the model can be built and tested on the same data.
































