---
title: "Forecasting Numeric Data - Regression Methods"
author: "Kevin D'Elia"
date: "June 10th, 2017"
output: 
  html_document: 
    highlight: kate
    number_sections: yes
    theme: spacelab
    toc: yes
  html_notebook: 
    highlight: pygments
    number_sections: yes
    theme: cosmo
    toc: yes
---
# Overview
Mathematical relationships help us to understand many aspects of everyday life.  For example, body weight is a function of one's calorie intake.  When such relationships are expressed with exact numbers, additional clarity is gained.  For example, an additional 250 kilo calories consumed daily may result in nearly a kilogram of weight gain per month.  This paper will cover:

* the basic statistical principles used in regression, a technique that models the size and the strength of numeric relationships
* how to prepare data for regression analysis, and estimate and interpret a regression model
* a pair of hybrid techniques known as regression trees and model trees, which adapt decision tree classifiers for numeric prediction tasks

# Understanding regression
Regression is concerned with specifying the relationship between a single numeric **dependent variable** (the value to be predicted) and one or more numeric **independent variables** (the predictors).  As the name implies, the dependent variable depends upon the value of the independent variable or variables.  The simplest forms of regression assume that the relationship between the independent and dependent variables follows a straight line.

Recall from basic algebra that lines can be defined in a **slope-interecept form ** similar to _y = a + bx_.  In this form, the letter _y_ indicates the dependent variable and _x_ indicates the independent variable.  The **slope** term _b_ specifies how much the line rises for each increase in _x_.  Positive values define lines that slope upward while negative values define lines that slope downward.  The term _a_ is known as the **intercept** because it specifies the point where the line crosses, or intercepts, the vertical _y_ axis.  It indicates the value of _y_ when _x = 0_.

Regression equations model data using a similar slop-intercept format.  The machine's job is to identify values of _a_ and _b_ so that the specified line is best able to relate the supplied _x_ values to the values of _y_.  There may not always be a single function that perfectly relates the values, so the machine must also have some way to quantify the margin of error.

Regression analysis is commonly used for modeling complex relationships among data elements, estimating the impact of a treatment on an outcome, and extrapolating into the future.

Regression methods are also used for **statistical hypothesis testing**, which determines whether a premise is likely to be true or false in light of the observed data.  The regression model's estimates of the strength and consistency of a relationship provide information that can be used to assess whether the observations are due to chance alone.

Regression analysis is not synonymous with a single algorithm.  Rather, it is an umbrella for a large number of methods that can be adapted to nearly any machine learning task.

This section focuses on only the most basic **linear regression** models - those that use straight lines.  When there is only a single independent variable it is known as **simple linear regression**.  In the case of two or more independent variables, this is known as **multiple linear regression**, or simply _multiple regression_.  Both of these techniques assume that the dependent variable is measure on a continuous scale.

Regression can also be used for other types of dependent variables and even for some classification tasks.  For instance, **logistic regression** is used to model a binary categorical outcome, while **Poisson regression** models integer count data.  The method known as **multinomial logistic regression** models a categorical outcome; thus, it can be used for classification.
Many of the specialized regression methods fall into a class of **Generalized Linear Models (GLM)**.  Using a GLM, linear models can be generalized to other patterns via the use of **link function**, which specifies more complex forms for the relationship between _x_ and _y_.  This allow regression to be applied to almost any type of data.

# Simple linear regression

A simple linear regression model defines the relationship between a dependent variable and a single independent predictor variable using a line defined by an equation in the following form:

_y = $\alpha +\beta x$_

The intercept alpha describes where the line crossed the _y_ axis, while the slope beta describes the change in _y_ given an increase of _x_.  Greek characters are often used in the field of statistics to indicate variables that are parameters of a statistical function.  Therefore, performing a regression analysis involves finding **paramter estimates** for _$\alpha$_ and _$\beta$_.  The parameter estimates for alpha and beta are often denoted using _a_ and _b_ although you may find that some of this terminology and notation is used interchangeably.

# Ordinary Least Squares Estimation
In order to determine the optimal estimates of _$\alpha$_ and _$\beta$_, an estimation method known as **Ordinary Least Squares (OLS)** was used.  In OLS regression, the slope and intercept are chosen so that they minimize the sum of the squared errors, that is, the vertical distance between the predicted _y_ value and the actual _y_ value.  These errors are known as **residuals**.  In mathematical terms, the goal of OLS regression can be expressed as the task of minimizing the following equation:

$\sum$_(y~i~ - $\hat{y}$~i~)^2^_ = $\sum$_e~i~^2^_

In plain language, this equation defines _e_ (the error)_ as the difference between the actual _y_ value and the predicted _y_ value.  The error values are squared and summed across all the points in the data.  The **y-hat** term is an estimate for the true _y_ value.

The solution for _a_ depends on the value of _b_.  It can be obtained using the following formula:

_a = $\bar{y}$ - b$\bar{x}$_

The horizontal bar appearing over the _x_ and _y_ terms indicates the mean value of _x_ or _y_.
The proof will not be given here but the value of _b_ that results in the minimum squared error is:

_$\frac{\sum(x_{i} - \bar{x})(y_{i} - \bar{y})}{\sum(x_{i} - \bar{x})^{2}}$_

This equation can be simplified by breaking it into its component parts.  The denominator is very similar to the **variance** of x.  The formula is:

_Var(x) = $\frac{\sum(x_{i} - \bar{x})^{2}}{n}$_

The numerator involves taking the sum of each data point's deviation from the mean _x_ value multiplied by that point's deviation away from the mean _y_ value.  This is similar to the **covariance** for _x_ and _y_; the formula is:

_Cov(x, y) = $\frac{\sum(x_{i} - \bar{x})(y_{i} - \bar{y})}{n}$_

Dividing covariance by variance cancels out the _n_ terms, leaving:

_b = $\frac{Cov(x, y)}{Var(x)}$_

The regression line can be estimated on the following dataset using built-in R function:
```{r slope}
launch <- read.csv("./challenger.csv")
b <- cov(launch$temperature, launch$distress_ct) / var(launch$temperature)
b
```
An estimation for _a_ can be achieved using the ```mean()``` function:
```{r intercept}
a <- mean(launch$distress_ct) - b * mean(launch$temperature)
a
```

# Correlations
The **correlation** between two variables is a number that indicates how closely their relationship follows a straight line; typically, this is **Pearson's correlation coefficient**.  The correlation ranges between -1 and +1, with extreme values indicating a perfectly linear relationship, while a correlation close to zero indicates the absence of a linear relationship.

The following formula defines Pearson's correlation:

**$\rho_{x,y}$ = _Corr(x, y)_ = $\frac{Cov(x, y)}{\sigma_{x}\sigma_{y}}$**

This formula can be used to calculate the correlation between the launch temperature and the number of distress events, or use the R correlation function:
```{r correlation}
r <- cov(launch$temperature, launch$distress_ct) /
       (sd(launch$temperature) * sd(launch$distress_ct))
r
cor(launch$temperature, launch$distress_ct)
```

The negative correlation implies that increases in temperature are related to decreases in the number of distressed O-rings.  The correlation also tells about the relative strength of the relationship between temperature and O-ring distress.  Because -0.51 is halfway to the maximum negative correlation of -1, this implies that there is a moderately strong negative linear association.

There are various rules of thumb used to interpret correlation strength.  One method assigns as status of "weak" to values between 0.1 and 0.3, "moderate" to the range of 0.3 to 0.5, and "strong" to values above 0.5 (these also apply to similar ranges of negative correlations).  However, more often the correlation must be interpreted in context.  "Correlation does not imply causation" means that a correlation only describes the association between a pair of variables, yet there could be other unmeasured explanations.  Measuring the correlation between two variables gives a way to quickly gauge the relationships among the independent and dependent variables.

# Multiple linear regression
Most real-world analyses have more than one independent variable, so **multiple linear regression** is typically used for most numeric prediction tasks.  It is an extension of simple linear regression, namely, to find values of beta coefficients that minimize the prediction error of a linear equation.  The key difference is that there are additional terms for additional independent variables.

Multiple regression equations generally follow the form of the following equation.  The dependent variable _y_ is specified as the sum of an intercept term _$\alpha$_ plus the product of the estimated _$\beta$_ value and the _x_ values for each of the _i_ features.  An error term _$\epsilon$_ has been added here as a reminder that the predictions are not perfect, and it represent the **residual** term noted previously:

_y = $\alpha$ + $\beta_{1}x_{1}$ + $\beta_{2}x_{2}$ + ... + $\beta_{i}x_{i}$ + $\epsilon$_

Consider for a moment the interpretation of the estimated regression parameters.  Note that in the preceding equation, a coefficient is provided for each feature.  This allows each feature to have a separate estimated effect on the value of _y_.  In other words, _y_ changes by the amount _$\beta_{i}$_ for each unit increase in _x~i~_.  The intercept _$\alpha$_ is then the expected value of _y_ when the independent variables are all zero.

Since the intercept term _$\alpha$_ is really no different than any other regression parameter, it is also sometimes denoted as _$\beta_{0}$_ as shown next:

_y = $\beta_{0}$ + $\beta_{1}x_{1}$ + $\beta_{2}x_{2}$ + ... + $\beta_{i}x_{i}$ + $\epsilon$_

Just like before, the intercept is unrelated to any of the independent _x_ variables.  However, for reasons that will become clear shortly, it helps to imagine _$\beta_{0}$_ as if it were being multiplied by a term _x~0^'^~_ which is a constant with the value 1:

_y = $\beta_{0}x_{0}$ + $\beta_{1}x_{1}$ + $\beta_{2}x_{2}$ + ... + $\beta_{i}x_{i}$ + $\epsilon$_

In order to estimate the values of the regression parameters, each observed value of the dependent variable _y_ must be related to the observed values of the independent _x_ variables using the regression equation in the previous form.  See pg.183 for a graphic of this relationship.  The many rows and columns of data illustrated can be described in a condensed formulation using bold font **matrix notation** to indicate that each of the terms represents multiple values:

**Y = $\beta$X + $\epsilon$**

The dependent variable is now a vector, **Y**, with a row for every example.  The independent variables have been combined into a matrix, **X**, with a column for each feature plus an additional column of '1' values for the intercept term.  Each column has a row for every example.  The regression coefficients **$\beta$** and residual errors **$\epsilon$** are also now vectors.

The goal is now to solve for **$\beta$**, the vector of regression coefficients that minimizes the sum of the squared errors between the predicted and actual **Y** values.  Finding the optimal solution requires the use of matrix algebra, but the best estimate of the vector **$\beta$** can be computed as:

**$\hat{\beta}$ = (X^T^X)^-1^X^T^Y**

This solution uses a pair of matrix operations - the **T** indicates the **transpose** of matrix **X**, while the negative exponent indicates the **matrix inverse**.  The following code create a basic regression function named ```reg()```, which takes a parameters _y_ and _x_ and returns a vector of estimated beta coefficients:

```{r beta_coefficients}
# creating a simple multiple regression function
reg <- function(y, x) {
  x <- as.matrix(x)
  x <- cbind(Intercept = 1, x)
  b <- solve(t(x) %*% x) %*% t(x) %*% y
  colnames(b) <- "estimate"
  print(b)
}
```

Since the function uses sets of columns from a data frame, the ```as.matrix()``` function is used to convert the data frame into matrix form.  The ```cbind()``` function is used to bind an additional column onto the _x_ matrix.  Then, a number of matrix operations are performed on the _x_ and _y_ objects:

* ```solve()``` takes the inverse of a matrix
* ```t()``` is used to transpose a matrix
* ```%*%``` multiplies two matrices

By combining these as shown, the function will return a vector _b_, which contains the estimated parameters for the linear model relating _x_ to _y_.  Applying the function to the shuttle launch data and then comparing it to the results achieved earlier with the simple linear model shows an exact match:
```{r compare_models}
# test regression model with simple linear regression
reg(y = launch$distress_ct, x = launch[2])
```

Now the function can be used to build a multiple regression model by specifying three columns of data instead of just one:
```{r multiple_regression}
# use regression model with multiple regression
reg(y = launch$distress_ct, x = launch[2:4])
```

# Example - predicting medical expenses using linear regression
In order for a health insurance company to make money, it needs to collect more in yearly premiums than it spends on medical care to its beneficiaries.  As a result, insurers invest a great deal of time and money in developing models that accurately forecast medical expenses for the insured population.

Medical expenses are difficult to estimate because the most costly conditions are rare and seemingly random.  Still, some conditions are more prevalent for certain segments of the population.  For instance, lung cancer is more likely among smokers than non-smokers, and heart disease may be more likely among the obese.
The goal of this analysis is to use patient data to estimate the average medical care expenses for such population segments.  These estimates can be used to create actuarial tables that set the price of yearly premiums higher or lower, depending on the expected treatment costs.

## Step 1 - collecting data

For this analysis, a simulated dataset containing hypothetical medical expenses for patients in the United States will be used; it is contained in the following file:
```{r read_medical_data}
insurance <- read.csv("insurance.csv", stringsAsFactors = TRUE)
```

The file includes 1,338 examples of beneficiaries currently enrolled in the insurance plan, with the following features:

* _age_: integer
* _sex_: factor
* _bmi_: body mass index
* _children_: integer
* _smoker_: factor
* _region_: factor

It is important to give some thought to how these variables may be related to billed medical expenses.  For instance, it is reasonable to expect that older people and smokers are at a higher risk of large medical expenses.  Unlike many other machine learning methods, in regression analysis, the relationships amount the features are typically specified by the user rather than being detected automatically.

## Step 2 - exploring and preparing the data

Now that the data has been read in, what is its structure?
```{r features}
str(insurance)
```

The model's dependent variable is _expenses_, which measures the medical costs each person charged to the insurance plan for the year.  Prior to building a regression model, it is often helpful to check for normality.  Although linear regression does not strictly require a normally distributed dependent variable, the model often fits better when this is true.  A quick look at the summary statistics reveals that the mean value is greater than the median, implying that the distribution of insurance expenses is right-skewed.  

```{r expense_summary}
summary(insurance$expenses)
```

A histogram will confirm this assumption:
```{r expense_histogram}
library(RColorBrewer)
hist(insurance$expenses, 
     xlab = "Expenses (in USD)", 
     ylab = "",
     col=brewer.pal(9, "Greens"),
     main = "Insurance Costs")
```

As expected, the figure shows a right-skewed distribution.  It also shows that the majority of people in the data have yearly medical expenses between zero and $15,000, in spite of the fact that the tail of the distribution extends far past these peaks.  Although this distribution is not ideal for a linear regression, knowing this weakness ahead of time may help to design a better-fitting model later on.

Before addressing that issue, another problem is at hand.  Regression models require that every feature is numeric, yet there are three factor-type features in the data frame.  One of those, _region_, has four levels, and this table shows the distribution of those levels as being fairly evenly divided:
```{r table_region}
table(insurance$region)
```

### Exploring relationships among features - the correlation matrix
Before fitting a regression model to data, it can be useful to determine how the independent variables are related to the dependent variable and each other.  A **correlation matrix** provides a quick overview of these relationships.  Given a set of variables, it provides a correlation for each pairwise relationship.  One for the four numeric variables in the insurance data appears next:
```{r cor_matrix}
# exploring relationships among features: correlation matrix
cor(insurance[c("age", "bmi", "children", "expenses")])
```

At the intersection of each row and column pair, the correlation is listed for the variables indicated by that row and column.  The diagonal is always 1.0 since there is always a perfect correlation between a variable and itself.  The values above and below the diagonal are identical since correlations are symmetrical.  In other words, _cor(x, y)_ is equal to _cor(y, x)_.

None of the correlations in the matrix are considered strong, but there are some notable associations.  For instance, _age_ and _bmi_ appear to have a weak positive correlation, meaning that as someone ages, their body mass tends to increase.  There is also a moderate positive correlation between _age_ and _expenses_, _bmi_ and _expenses_, and _children_ and _expenses_.  These associations imply that as age, body mass, and number of children increase, the expected cost of insurance goes up.

### Visulaizing relationships among features - the scatterplot matrix
It can also be helpful to visualize the relationships among numeric features by using a scatterplot.  Although a scatterplot for each possible relationship could be created, doing so for a large number of features might become tedious.

An alternative is to create a **scatterplot matrix**, or **SPLOM**, which is simply a collection of scatterplots arranged in a grid.  It is used to detect patterns among three or more variables.  The scatterplot matrix is not a true multidimensional visualization because only two features are examined at a time.  Still, it provides a general sense of how the data may be interrelated.  The ```pairs()``` function provides basic functionality for producing scatterplot matrices:
```{r pairs}
# visualing relationships among features: scatterplot matrix
pairs(insurance[c("age", "bmi", "children", "expenses")])
```

In the scatterplot matrix, the intersection of each row and column holds the scatterplot of the variables indicated by the row and column pair.  The diagrams above and below the diagonal are transpositions since the _x_ axis and _y_ axis have been swapped.

Although some of the plots look like random clouds of points, a few seem to display some trends.  The relationships between _age_ and _expenses_ displays several relatively straight lines, while the _bmi_ versus _expenses_ plot has two distinct groups of points.  It is difficult to detect trends in any of the other plots.

If we add more information to the plot, it can be even more useful; the ```pairs.panel()``` function in the _psych_ package does just that:
```{r psych}
library(psych)
pairs.panels(insurance[c("age", "bmi", "children", "expenses")])
```

Above the diagonal, the scatterplots have been replaced with a correlation matrix.  On the diagonal, a histogram depicting the distribution of values for each feature is shown.  Finally, the scatterplots below the diagonal are now presented with additional information.

The oval-shaped object on each scatterplot is a **correlation ellipse**.  It provides a visualization of correlation strength.  The dot at the center of the ellipse indicates the point at the mean values for the _x_ and _y_ axis variables.  The correlation between the two variables is indicated by the shape of the ellipse; the more it is stretched, the stronger the correlation.  An almost perfectly round oval, as with _bmi_ and _children_, indicates a very weak correlation (in this case, it is 0.01).

The curve drawn on the scatterplot is called a **loess curve**. It indicates the general relationship between the _x_ and _y_ axis variables.  It is best understood by an example.  The curve for _age_ and _children_ is an upside-down U, peaking around middle age.  This means that the oldest and youngest people in the sample have fewer children on the insurance plan than those around middle age.  Because this trend is non-linear, this finding could not have been inferred from the correlations alone.  On the other hand, the loess curve for _age_ and _bmi_ is a line sloping gradually up, implying that body mass increases with age, but this was already inferred from the correlation matrix.

## Step 3 - training a model on the data
To fit a linear regression model to data with R, the ```lm()``` function can be used.  The ```lm()``` syntax is as follows:

**Building the model:**

```m <- lm(dv ~ iv, data = mydata)```

where

* _dv_ is the dependent variable in the **mydata** data frame to be modeled
* _iv_ is an R formula specifying the independent variables in the **mydata** data frame to use in the model
* _data_ specifies the data frame in which the **dv** and **iv** variables can be found

The function will return a regression model object that can be used to make predictions.  Interactions between independent variables can be specified using the * operator.

**Making predictions:**

```p <- predict(m, test)```

where

* _m_ is a model trained by the **lm()** function
* _test_ is a data frame containing test data with the same features as the training data used to build the model

The function will return a vector of predicted values.

**Example:**

```ins_model <- lm(charges ~ age + sex + smoker, data = insurance)```

```ins_predict <- predict(ins_model, insurance_test)```

The following command fits a linear regression model relating the six independent variables to the total medical expenses.  The R formula syntax uses the tilde character to describe the model; the dependent variable ```expenses``` goes to the left of the tilde while the independent variables go to the right, separated by + signs.  There is no need to specify the regression model's intercept term as it is assumed by default:

```ins_model <- lm(expenses ~ age + children + bmi + sex + smoker + region, data = insurance)```

Because the . character can be used to specify all the features (excluding those a;ready specified in the formula), the following command is equivalent to the preceding command (with resulting model data shown):

```{r model}
ins_model <- lm(expenses ~ ., data = insurance)
ins_model
```

Understanding the regression coefficients is fairly straightforward.  The intercept is the predicted value of ```expenses``` when the independent variables are equal to zero.  As is the case here, quite often the intercept is of little value alone because it is impossible to have values of zero for all features.  Fore example, since no person exists with age zero and BMI zero, the intercept has no real-world interpretation.  For this reason, in practice, the intercept is often ignored.

The beta coefficients (the ones used for multiplying the independent variables) indicate the estimated increase in expenses for an increase of one in each of the features, assuming all other values are held constant.  For instance, for each additional year of age, medical expenses would increase by $256.80 on average, assuming all other factors being equal.  Similarly, each additional child results in an average of $475.70 in additional medical expenses each year, and each unit increase in BMI is associated with an average increase of $339.30 in yearly medical expenses, all else equal.

Although only six features were specified in the model formula, eight coefficients are reported in addition to the intercept.  This happened because the ```lm()``` function automatically applied a technique known as **dummy coding** to each of the factor-type variables included in the model.

Dummy coding allows a nominal feature to be treated as numeric by creating a binary variable, often called a **dummy variable**, for each category of the feature.  The dummy variable is set to _1_ if the observation falls into the specified category or _0_ otherwise.  For instance, the _sex_ feature has two categories: _male_ and _female_. This will be split into two binary variables, which R names _sexmale_ and _sexfemale_.  For observations where _sex = male_, then _sexmale = 1_ and _sexfemale = 0_, and vice versa.  The same coding applies to variables with three or more categories, such as _region_, which was split into _regionnorthwest_, etc.

When adding a dummy variable to a regression model, one category is always left out to serve as the reference category.  The estimates are then interpreted relative to the reference.  In this model, R automatically held out the _sexfemale_, _smokerno_, and _regionnortheast_ variables, making female non-smokers in the northeast region the reference group.  Thus, makes have 4151,40 less medical expenses each year relative to females and smokers cost an average of $23,847.50 more than non-smokers per year.  The coefficient for each of the three regions in the model is negative, which implies that the reference group, the northeast region, tends to have the highest average expenses.

By default, R uses the first level of the factor variable as the reference.  To use another level, the ```relevel()``` function can be used to specify the reference group manually.

The results of the linear regression model make logical sense:  old age, smoking, and obesity tend to be linked to additional health issues, while additional family member dependents may result in an increase in physician visits and preventive care such as vaccinations and yearly physical exams.  There is no indication at this point, however, of how well the model is fitting the data.  The next section will address this point.

## Step 4 - evaluating model performance
The parameter estimates we obtained by typing ```ins_model``` tell about how the independent variables are related to the dependent variable, but they tell us nothing about how well the model fits our data.  The ```summary()``` command will do that:
```{r model_summary}
summary(ins_model)
```

The output provides three key ways to evaluate the performance, or fit, of the model:

1. The **residuals** section provides summary statistics for the _errors_ in the predictions, some of which are apparently quite substantial.  Since a residual is equal to the true value minus the predicted value, the maximum error of 29981.7 suggests that the model under-predicted expenses by nearly $30,000 for at least one observation, i.e., true value = $100,000 and predicted value = $70,000.  On the other hand, 50% of errors fall within the 1Q and 3Q values, so the majority of predictions were between $2,850,90 over the true value and $1383.90 under the true value.  So, negative values imply over-prediction and positive values imply under-prediction.

2. For each estimated regression coefficient, the **p-value**, denoted ```Pr(>|t|)```, provides an estimate of the probability that the true coefficient is zero given the value of the estimate.  Small p-values suggest that the true coefficient is very unlikely to be zero, which means that the feature is extremely unlikely to have no relationship with the dependent variable.  Note that some of the p-values have stars, which correspond to the footnotes to indicate the **significance level** met by the estimate.  This level is a threshold, chosen prior to building the model, which will be used to indicate "real" findings, as opposed to those due to chance alone; p-values less than the significance level are considered **statistically significant**.  If the model had few such terms, it may be cause for concern, since this would indicate that the features used are not very predictive of the outcome.  Here, the model has several highly significant variables, and they seem to be related to the outcome in logical ways.

3. The **multiple R-squared value** (also called the _coefficient of determination_) provides a measure of how well the model as a whole explains the values of the dependent variable.  It is similar to the correlation coefficient, in that the closer the value is to 1.0, the better the model perfectly explains the data.  Since the R-squared values is 0.7494, we know that the model explains nearly 75% of the variation in the dependent variable.  Because models with more features always explain more variation, the **adjusted R-squared value** corrects R-squared by penalizing models with a large number of independent variables.  It is useful for comparing the performance of models with different numbers of explanatory variables.

Given the preceding three performance indicators, the model is performing fairly well.  It is not uncommon for regression models of real-world data to have fairly low R-squared values, a value of 0.75 is actually quite good.  The size of some of the error sis a bit concerning, but not surprising given the nature of medical expense data.

## Step 5 - improving model performance
As mentioned previously, a key difference between the regression modeling and other machine learning approaches is that regression typically leaves feature selection  and model specification to the user.  Consequently, if there is subject matter knowledge about how a feature is related to the outcome, this information can be used to inform the model specification and potentially improve the model's performance.

### Model specification - adding non-linear relationships
In linear regression, the relationship between an independent variable and the dependent variable is assumed to be linear, yet this may not necessarily be true.  For example, the effect of age on medical expenditure may not be constant throughout all the age values; the treatment may become disproportionately expensive for oldest populations.

Recall that a typical regression equation follows a form similar to this:

**_y = $\alpha$ + $\beta_{1}$x_**

To account for a non-linear relationship, add a higher order term to the regression model, treating the model as a polynomial.  In effect, it is modeling a relationship like this:

**_y = $\alpha$ + $\beta_{1}$x + $\beta_{2}$x^2^_**

The difference between these two models is that an additional beta will be estimated, which is intended to capture the effect of the _x_-squared term.  This allows the impact of age to be measured as a function of age squared.

To add the non-linear age to the model, simply create a new variable:

```{r age2}
insurance$age2 <- insurance$age^2
```

The new, improved model will have both ```age``` and ```age2``` in the _lm()_ formula using the ```expenses ~ age + age2``` form.  This allows the model to separate the linear and on-linear impact of age on medical expenses.

### Transformation - converting a numeric variable to a binary indicator
By creating a binary obesity indicator variable that is 1 if the BMI is at least 30, and 0 if less, the relationship between BMI and expenses can be modeled.  The estimated beta for this binary feature would then indicate the average net impact on medical expenses for individuals with BMI of 30 or above, relative to those with BMI less than 30.

The ```ifelse()``` function can be used for this purpose, for each element in a vector a test against a specified condition is performed, as shown next:

```{r ifelse}
insurance$bmi30 <- ifelse(insurance$bmi >= 30, 1, 0)
```

As before with _age2_, the _bmi30_ variable can be included in the new, improved model; it will either replace or complement the existing _bmi_ variable, depending on the needs of the model.  In other words, does obesity have any effect in addition to the separate linear BMI effect.  To determine whether or not to include the variable in the model, examine the p-value which will indicate the statistical significance of the variable.

### Model specification - adding interaction effects
So far, only each feature's individual contribution to the outcome has been considered.  What if certain features have a combined impact on the dependent variable?  For instance, smoking and obesity may have harmful effects separately, but it is reasonable to assume that their combined effect may be worse than the sum of each one alone.

When two features have a combined effect, this is known as an **interaction**.  If it is thought that two variables interact, this hypothesis can be tested by adding their interaction to the model.  Interaction effects are specified using the R formula syntax.  To have the obesity indicator _bmi30_ and the smoking indicator _smoker_ interact, write the formula in the form ```expenses ~ bmi30*smoker```.

The * operator is shorthand that instructs R to model ```expenses ~ bmi30 + smokeryes + bmi30:smokeryes```.  The colon operator in the expanded form indicates that the ```bmi30:smokeryes``` is the interaction between the two variables.  Note that the expanded form also automatically included the _bmi30_ and _smoker_ variables as well as the interaction.  Interactions should never be included in a model without also adding each of the interacting variables.

### Putting it all together - an improved regression model
To summarize the improvements:

* added a non-linear term for age
* created an indicator for obesity
* specified an interaction between obesity and smoking

The new model will include the newly constructed variables and the interaction term:
```{r new_model}
# create final model
ins_model2 <- lm(expenses ~ age + age2 + children + bmi + sex +
                   bmi30*smoker + region, data = insurance)

summary(ins_model2)
```

The model fit statistics help to determine whether the changes improved the performance of the regression model.  Relative to the first model, the R-squared value has improved from 0.75 to about 0.87.  Similarly, the adjusted R-squared value, which takes into account the fact that the model grew in complexity, also improved with the same numbers.  The model is now explaining 87% of the variation in medical treatment costs.  Additionally, the theories about the model's functional form seem to be validated.  The higher-order ```age2``` term is statistically significant, as is the obesity indicator ```bmi30```.  The interaction between obesity and smoking suggests a massive effect; in addition to the increased costs of over $13,404 for smoking alone, obese smokers spend another $19,810 per year.  This may suggest that smoking exacerbates diseases associated with obesity.

# Understanding regression trees and model trees
Recall that a decision tree builds a model much like a flowchart in which decision nodes, leaf nodes, and branches define a series of decisions that are used to classify examples.  Such trees can also be used for numeric prediction by making only small adjustments to the tree-growing algorithm.  This section will consider only the ways in which trees for numeric prediction differ from trees used for classification.

Trees for numeric prediction fall into two categories.  The first known as **regression trees**, were introduced in the 1980s as part of the seminal **Classification and Regression Tree (CART)** algorithm.  Despite the name, regression trees do not use linear regression methods as described earlier in this chapter; rather, they make predictions based on the average value of examples that reach a leaf.

The second type of trees for numeric prediction are known as **model trees**.  Introduced several years later than regression trees, they are lesser-known, but perhaps more powerful.  Model trees are grown in much the same way as regression trees, but at each leaf, a multiple linear regression model is built from the examples reaching that node.  Depending on the number of leaf nodes, a model tree may build tens or even hundreds of such models.  This may make model trees more difficult to understand than the equivalent regression tree, with the benefit that they may result in a more accurate model.

# Adding regression to trees
Trees that can perform numeric prediction offer a compelling yet often overlooked alternative to regression modeling.  The strengths and weaknesses of regression trees and model trees relative to the more common regression methods are listed on page 202 of the book.

Though traditional regression methods are typically the first choice for numeric prediction tasks, in some cases, numeric decision trees offer distinct advantages.  For instance, decision trees may be better suited for tasks with many features or many complex, non-linear relationships among features and outcome.  These situations present challenges for regression. Regression modeling also makes assumptions about how numeric data is distributed that are often violated in real-world data.  This is not the case for trees.

Trees for numeric prediction are built in much the same way as they are for classification.  Beginning at the root node the data is partitioned using a divide-and-conquer strategy according to the feature that will result in the greatest increase in homogeneity in the outcome after a split is performed.  In classification trees, recall that homogeneity is measured by entropy, which is undefined for numeric data.  Instead, for numeric decision trees, homogeneity is measured by statistics such as variance, standard deviation, or absolute deviation from the mean.

One common splitting criteria is called the **Standard Deviation Reduction (SDR)**.  It is defined by the following formula:

  **SDR = _sd(T)_ -  $\Sigma_{i}$ $\frac{|T_{i}|}{|T|}$ $\times$ _sd(T~i~)_**

In this formula, the _sd(T)_ function refers to the standard deviation of the values in set _T_, while _T~1~, T~2~, $\ldots$, T~n~_ are the sets of values resulting from a split on a feature.  The _|T|_ term refers to the number of observations in set _T_.  Essentially, the formula measures the reduction in standard deviation by comparing the standard deviation pre-split to the weighted standard deviation post-split.

Consider the following case in which a tree is deciding whether or not to perform a split on binary feature A or B.  Using the groups that would result from a proposed split, the SDR for both A and B is computed as follows:
```{r sdr}
## Example: Calculating SDR ----
# set up the data
tee <- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7)
at1 <- c(1, 1, 1, 2, 2, 3, 4, 5, 5)
at2 <- c(6, 6, 7, 7, 7, 7)
bt1 <- c(1, 1, 1, 2, 2, 3, 4)
bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7)

# compute the SDR
sdr_a <- sd(tee) - (length(at1) / length(tee) * sd(at1) + length(at2) / length(tee) * sd(at2))
sdr_b <- sd(tee) - (length(bt1) / length(tee) * sd(bt1) + length(bt2) / length(tee) * sd(bt2))

# compare the SDR for each split
sdr_a
sdr_b
```

The SDR for the split on feature A was about 1.2 versus 1.4 for the split on feature B.  Since the standard deviation was reduced more for the split on B, the decision tree would use B first.  It results in slightly more homogeneous sets than with A.

Suppose that the tree stopped growing here using this one and only split.  A regression tree's work is done.  It can make predictions for new examples depending on whether the example's value on feature B places the example into group _T~1~_ or _T~2~_.  If the example ends up in _T~1~_, the model would predict _mean(bt1) = 2_, otherwise it would predict _mean(bt2) = 6.25_.

In contrast, a model tree would go one step further.  Using the seven training examples falling in group _T~1~_ and the eight in _T~2~_, the model tree could build a linear regression model of the outcome versus feature A.  Note that Feature B is of no help in building the regression model because all examples at the leaf have the same value of B - they were placed into _T~1~_ or _T~2~_ according to their value of B.  The model tree can then make predictions for new examples using either of the two linear models.

# Example - estimating the quality of wines with regression trees and model trees
This case study will use regression trees and model trees to create a system capable of mimicking expert ratings of wine.  Because trees result in a model that is readily understood, this can allow the winemakers to identify the key factors that contribute to better-rated wines.  Perhaps more importantly, the system does not suffer from the human elements of tasting, such as the rater's mood or palate fatigue.

## Step 1 - collecting data
The data used for this analysis will be based on a white wine dataset of white Vinho Verde wines from Portugal.  The dataset for red wines is included for later analysis.

The white wine data includes information on 11 chemical properties of 4,898 wine samples.  For each wine, a laboratory analysis measured characteristics such as acidity, sugar content, chlorides, sulfur, alcohol, pH, and density.  The samples were then rated in a blind tasting by panels of no less than three judges on a quality scale ranging from zero (very bad) to 10 (excellent).  In the case of judges disagreeing on the rating, the median value was used.

## Step 2 - exploring and preparing the data
The data is loaded in the typical fashion, however, as all of the features are numeric, ```stringsAsFactors``` need not be specified.
```{r white_wine}
wine <- read.csv("whitewines.csv")

str(wine)
```

Compared with other types of machine learning models, one of the advantages of trees is that they can handle many types of data without preprocessing.  This means that the features do not need to be normalized or standardized.

However, a bit of effort to examine the distribution of the outcome variable is needed to inform the evaluation of the model's performance.  For instance, suppose that there was a very little variation in quality from wine-to-wine, or that wines fell into a bimodal distribution - either very good or very bad.  To check for such extremes, the distribution of quality can be examined with a histogram:
```{r wine_historgram}
library(RColorBrewer)
hist(wine$quality, col=brewer.pal(9, "BrBG"), xlab = "Low -> High", ylab = "", main = "Wine Quality")
```

The wine quality values appear to follow a fairly normal, bell-shaped distribution. centered around a value of six.  This makes sense intuitively because most wines are of average quality; few are particularly god or good.

The last step is to divide into training and testing data sets.  Since the **wine** dataset was already sorted into random order, it can be partitioned into two sets of contiguous rows as follows:
```{r wine_splits}
wine_train <- wine[1:3750, ]
wine_test <- wine[3751:4898, ]
```

## Step 3 - training a model on the data
Training a regression tree model using the ```rpart``` package is the approach which uses the most faithful implementation of regression trees.

**Building the model**

```m <- rpart(dv ~ iv, data = mydata)```

where:

* _dv_ is the dependent variable in the **mydata** data frame to be modeled.

* _iv_ is an $ formula specifying the independent variables in the **mydata** data frame to use in the model

* _data_ specifies the data frame in which the **dv** and **iv** variables can be found

The function  will return a regression tree model object that can be used to make predictions.

**Making predictions**

```p <- predict(m, test, type = "vector")```

where:

* _m_ is a model trained by the **rpart()** function

* _test_ is a data frame containing test data with the same features as the training data used to build the model

*  _type_ specifies the type of prediction to return, either "vector" (for predicted numeric values), "class" for predicted classes, or "prob" (for predicted class probabilities)

The function will return a vector of predictions depending on the **type** parameter.

Using the R formula interface, _quality_ can be specified as the outcome variable and the dot notation allow all the other columns in the **wine_train** data frame to be used as predictors.  The resulting regression tree model object is named ```m.rpart``` to distinguish it from the model tree that will be trained later:
```{r m_rpart}
library(rpart)
m.rpart <- rpart(quality ~ ., data = wine_train)
```

To get basic information about the tree, just type the name of the model object:
```{r m_rpart_info}
m.rpart
```

For each node in the tree, the number of examples reaching the decision point is listed.  For instance, all 3,750 examples begin at the root node, of which 2,372 have _alcohol < 10.85_ and 1,378 have _alcohol_ >= 10.85.  Because alcohol was used first in the tree, it is the single most important predictor of wine quality.

Nodes indicated by * are terminal or leaf nodes, which means that they result in a prediction (listed here as _yval_).  For example, node 5 has a _yval_ of 5.971091.  When the tree is used for predictions, any wine samples with _alcohol < 10.85_ and _volatile.acidity < 0.2275_ would therefore be predict to have a quality value of 5.97.

A more detailed summary of the tree's fit, including the mean squared error for each of the nodes and an overall measure of feature importance, can be obtained using the ```summary(m.rpart)``` command.

### Visualizing decision trees

Although the tree can be understood using only the preceding output, it is more readily understood using visualization through the ```rpart.plot``` package.
```{r rpart_model}
library(rpart.plot)
rpart.plot(m.rpart, digits = 3)
```

Using the ```fallen.leaves``` parameter forces the leaf nodes to be aligned at the bottom of the plot, while the ```type``` and ```extra``` parameters affect the way the decisions and nodes are labeled.

```{r m_rpart_tree}
rpart.plot(m.rpart, digits = 4, fallen.leaves = TRUE, type = 3, extra = 101)
```
Visualizations like these may assist with the dissemination of regression tree results.  In both cases, the numbers shown in the leaf nodes are the predicted values for the examples reaching that node.

## Step 4 - evaluating model performance

The ```predict()``` function will use the model tree to make predictions on the test data.  By default, this returns the estimated numeric value for the outcome variable.
```{r tree_predict}
p.rpart <- predict(m.rpart, wine_test)
```

A quick look at the summary statistics of the predictions suggests a potential problem; the predictions fall on a much narrower range than the true values:
```{r predict_summaries}
# compare the distribution of predicted values vs. actual values
summary(p.rpart)
summary(wine_test$quality)
```

This finding suggests that the model is not correctly identifying the extreme cases, in particular the best and worst wines.  On the other hand, between the first and third quartile, the model may be doing quite well.

The correlation between the predicted and actual quality values provides a simple way to gauge the model's performance.
```{r model_cor}
cor(p.rpart, wine_test$quality)
```

This correlation value is certainly acceptable.  However, the correlation only measures how strongly the predictions are related to the true value; it is not a measure of how far off the predictions were from the true values.

### Measuring performance with the mean absolute error

Another way to think about the model's performance is to consider how far, on average, its prediction was from the true value.  This measurement is called the **mean absolute error (MAE)**.  The equation for MAE is as follows, where _n_ indicates the number of predictions and _e~i~_ indicates the error for prediction _i_:

**MAE = $\frac{1}{n}\Sigma_{i=1}^n$|_e~i~_|**

As the name implies, this equation takes the mean of the absolute value of the errors.  Since the error is just the difference between the predicted and actual values, a simple MAE() function can be created as follows:
```{r MAE}
# function to calculate the mean absolute error
MAE <- function(actual, predicted) {
  mean(abs(actual - predicted))  
}
```

Using the function to determine the MAE for the predictions:
```{r MAE_eval}
# mean absolute error between predicted and actual values
MAE(p.rpart, wine_test$quality)
```

This implies that, on average, the difference between the model's predictions and the true quality score was about 0.59.  On a quality scale from zero to 10, this seems to suggest that the model is doing fairly well.

On the other hand, recall that most wines were neither very good nor very bad; the typical quality score was around five to six.  Therefore, a classifier that did nothing but predict the mean value may still do fairly well according to this metric.

The mean quality rating in the training data is as follows:
```{r mean_training}
mean_quality <- mean(wine_train$quality) 
```

If this value was predicted for every wine sample, this would give an MAE of ~0.67:
```{r}
round(MAE(mean_quality, wine_test$quality), 2)
```
The regression tree value of 0.59 comes closer on average to the true quality score than the imputed mean of 0.67 but not by much.

## Step 5 - improving model performance
Building a model tree may improve the performance of the learner.  Recall that a model tree improves on regression trees by replacing the leaf nodes with regression models.  This often results in more accurate results than regression trees, which use only a single value for prediction at the leaf nodes.

The current state-of-the-art in model trees is the **M5' algorithm (M5-prime)** and is included in the _RWeka_ package through the _MP5()_ function.

**Building the model**

```m <- M5P(dv ~ iv, data = mydata)```

where:

* _dv_ is the dependent variable in the **mydata** data frame to be modeled

* _iv_ is an R formula specifying the independent variables in the **mydata** data frame to use in the model
 
* _data_ specifies the data frame in which the **dv** and **iv** variables can be found

The function will return a model tree object that can be used to make predictions

**Making Predictions**

```p <- predict(m, test)```

where:

* _m_ is a model trained by the **M5P()** function

* _test_ is a data frame containing test data with the same features as the training data used to build the model

The function will return a vector of predicted numeric values.

**Example:**
```
wine_model <- M5P(quality ~ alcohol + sulfates, data = wine_train)
wine_predictions <- predict(wine_model, wine_test)
```
Fit the model tree using essentially the same syntax as was used for the regression tree:
```{r RWeka}
library(RWeka)
m.m5p <- M5P(quality ~ ., data = wine_train)
```

The tree itself can be examined by typing its name.  As the output is very large, that step is not included here.  Basically, here is the information presented:

* The splits are very similar to the regression tree that was built earlier

* Alcohol is the most important variable, followed by volatile acidity and free sulfur dioxide.  A key difference, however, is that the nodes terminate not in a numeric prediction, but a linear model (shown as _LM1_ and _LM2_)

* The linear models are shown later in the output.  For instance, the model for _LM1_ has values which can be interpreted exactly the same as the multiple regression models built ealier.  Each number is the net effect of the associated feature on the predicted wine quality.  The coefficient of 0.266 for fixed acidity implies that for an increase of 1 unit of acidity, the wine quality is expected to increase by 0.266

* It is important to note that the effects estimated by _LM1_ apply only to wine samples reaching this node; a totla of 36 linear models were built in this model tree, wach with different estimates of the impact of fixed acidity and the other 10 features.

For statistics on how well the model fits the training data, the _summary()_ function can be applied to the M5P model.  However, note that since these statistics are based on the training data, they should be used only as a rough diagnostic:
```{r m5p_summary}
summary(m.m5p)
```

Instead, it is more useful to see how well the model performs on the unseen test data; the _predict()_ function returns a vector of predicted values:
```{r m5p_predict}
p.m5p <- predict(m.m5p, wine_test)
```

The model tree appears to be predicting a wider range of values than the regression tree:
```{r summary_m5p_predict}
summary(p.m5p)
```

And the correlation also seems to be substantially higher:



The correlation also seems to be substantially higher:
```{r predict_correlation}
cor(p.m5p, wine_test$quality)
```

Furthermore, the model has slightly reduced the mean absolute error:
```{r m5p_mae}
# mean absolute error of predicted and true values
# (uses a custom function defined above)
MAE(wine_test$quality, p.m5p)
```

















