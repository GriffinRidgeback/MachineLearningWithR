---
title: "Probabilistic Learning - Classification using Naive Bayes"
author: "Kevin D'Elia"
date: "April 29th, 2017"
output: 
  html_notebook: 
    highlight: pygments
    number_sections: yes
    theme: cosmo
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Understanding Naive Bayes
Thomas Bayes developed foundational principles to describe the probability of events, and how probabilities should be revised in the light of additional information.  These principles formed the foundation for what are now known as **Bayesian methods**.

Classifiers based on Bayesian methods utilize training data to calculate an observed probability of each outcome based on the evidence provided by feature values.  When the classifier is later applied to unlabeled data, it uses the observed probabilities to predict the most likely class for the new features.

Bayesian classifiers are useful for:

* Text classification, such as junk e-mail (spam) filtering
* Intrusion or anomaly detection in computer networks
* Diagnosing medical conditions given a set of observed symptoms

Typically, Bayesian classifiers are best applied to problems in which the information from numerous attributes should be considered simultaneously in order to estimate the overall probability of an outcome.  While many machine learning algorithms ignore features that have weak effects, Bayesian methods utilize all the available evidence to subtly change the predictions.  If large number of features have relatively minor effects, taken together, their combine impact could be quite large.

## Basic concepts of Bayesian methods

Bayesian probability theory is rooted in the idea that the estimated likelihood of an **event**, or a potential outcome, should be based on the evidence at hand across multiple **trials**, or opportunities for the event to occur.

```{r bayes_table, echo=FALSE}
library(knitr)

events <- c("Heads result", "Rainy weather", "Message is spam", "Candidate becomes president", "Win the lottery")
trials <- c("Coin flip", "A single day", "Incoming e-mail message", "Presidential election", "Lottery ticket")
df <- data.frame(events, trials)
column.names <- c("Event", "Trial")
kable(df, col.names = column.names)
```
## Understanding probability

The probability of an event is estimated from the observed data by dividing the number of trials in which the event occurred by the total number of trials.  To denote these probabilities, we use notation in the form _P(A)_ which signifies the probability of event _A_.

The probability of all the possible outcomes of a trial must always sum to 1, because a trial always results in some outcome happening.  Thus, if the trial has two outcomes that cannot occur simultaneously, such as rainy versus sunny or spam versus ham, then knowing the probability of either outcome reveals the probability of the other.  For example, given the value _P(spam) = 0.20_, then _P(ham) = 1 - 0.20 = 0.80_.  This concludes that spam and ham are **mutually exclusive and exhaustive** events, which implies that they cannot occur at the same time and are the only possible outcomes.

Because an event cannot simultaneously happen and not happen, an event is always mutually exclusive and exhaustive with its **complement**, or the event comprising of the outcomes in which the event of interest does not happen.  The complement of event _A_ is typically denoted _A^c^_ or _A^'^_.  Additionally, the shorthand notation _P(A^c^)_ denotes the probability of event _A_ not occurring.

## Understanding joint probability
Often, we are interested in monitoring several non-mutually exclusive events for the same trial.  If certain events occur with the event of interest, we may be able to use them to make predictions.  Consider, for instance, a second event based on the outcome that an e-mail message contains a particular word that would most likely appear only in a spam message; its presence in an incoming e-mail is therefore a very strong piece of evidence that the message is spam.  If it is known that 20% of all messages were spam and 5% of all messages contained the word in question, the goal is to estimate the probability that both _P(spam)_ and _P(word)_ occur; this is written as _P(spam $\cap$word)_.  This notation signifies the **intersection** of the two events, or to the event where **both** _A_ and _B_ occur.

Calculating _P(spam $\cap$word)_ depends on the **joint probability** of the two events or how the probability of one event is related to the probability of the other.  If the two events are totally unrelated, the are called **independent events**.  This is not to say that independent events cannot occur at the same time; event independence simply implies that knowing the outcome of one event does not provide any information about the outcome of the other.  For instance, the outcome of a heads result on a coin flip is independent from whether the weather is rainy or sunny on any given day.

If all events were independent, it would be impossible to predict one event by observing another.  In other words, **dependent events** are the basis of predictive modeling.  Calculating the probability of dependent events is a bit more complex than for independent events.  If _P(spam)_ and _P(word)_ were independent, calculating _P(spam $\cap$word)_, the probability of both events happening at the same time, is easy.  It is simply _0.05 * 0.20 = 0.01_.  More generally, for independent events _A_ and _B_, the probability of both happening can be expressed as _P(spam $\cap$word) = P(A) * P(B)_.  But it is more likely that the two probabilities are highly dependent, which requires a more careful formulation of the relationship between the two events based on advanced Bayesian methods.

## Computing conditional probability with Bayes' theorem

**Bayes' theorem**: describes the relationship between dependent events; a way of thinking about how to revise an estimate of the probability of one event in light of the evidence provided by another event:

_P(A $\mid$ B) = $\frac{P(A \cap B)}{P(B)}$_

The notation _P(A $\mid$ B)_ is read as the probability of event _A_, given that event _B_ occurred.  This is known as **conditional probability**, since the probability of _A_ is dependent (that is, conditional) on what happened with event _B_.  Bayes' theorem tells us that our estimate of _P(A $\mid$ B)_ should be based on _P(A $\cap$ B)_, a measure of how often _A_ and _B_ are observed to occur together, and _P(B)_, a measure of how often _B_ is observed to occur in general.

Bayes' theorem states that the best estimate of _P(A $\mid$ B)_ is the proportion of trials in which _A_ occurred with _B_ out of all the trials in which _B_ occurred.  In plain language, this means that if there is knowledge that event _B_ occurred, the probability of event _A_ is higher the more often that _A_ and _B_ occur together each time _B_ is observed.  In a way, this adjusts _P(A $\cap$ B)_ for the probability of _B_ occurring; if _B_ is extremely rate, _P(B)_ and _P(A $\cap$ B)_ will always be small; however, if _A_ and _B_ almost always happen together, _P(A $\mid$ B)_ will be high regardless of the probability of _B_.

By definition, _P(A $\cap$ B) = P(A $\mid$ B) $\times$ P(B)_ by algebra; rearrange according to _P(A $\cap$ B) = P(B $\cap$ A)_ with resulting conclusion that _P(A $\cap$ B) = P(B $\mid$ A) $\times$ P(A)_ (swapping B's and A's).  This leads to:

_P(A $\mid$ B) = $\frac{P(A \cap B)}{P(B)}$ = $\frac{P(B \mid A) \times P(A)}{P(B)}$_

Continuing with the example, the best estimate of _P(spam)_ was 20%; this is known as the **prior probability**.  Looking more carefully at the set of previously received messages to examine the frequency of the term in question results in the probability that this word was used in previous spam messages, or _P(word $\mid$ spam)_,  is called the **likelihood**.  The probability that the word appeared in any message at all, or _P(word)_, is known as the **marginal likelihood**.

By applying Bayes' theorem to this evidence, we can compute a **posterior probability** that measures how likely the message is to be spam.  If the posterior probability is greater than 50%, the message is more likely to be spam.  The following lists shows how the definitions given map to the components of the Bayes' theorem:

**_P(A $\mid$ B) = $\frac{P(B \mid A) \times P(A)}{P(B)}$_**

**posterior probability** = _P(A $\mid$ B)_

**likelihood**            = _P(B $\mid$ A)_

**prior probability**      = _P(A)_

**marginal likelihood**   = _P(B)_

To calculate these components of Bayes' theorem, it helps to construct a **frequency table** that records the number of times the word appeared in spam and ham messages.  Just like a two-way cross-tabulation, one dimension of the table indicates levels of the class variable (spam or ham), while the other dimension indicates levels for features (word: yes or no).  The cells then indicate the number of instances having the particular combination of class value and feature value.  The frequency table can then be used to construct a **likelihood table**.  The rows of the likelihood table indicate the conditional probabilities for the word (yes/no), given that an e-mail was ham or spam.

# The Naive Bayes algorithm

This algorithm describes a simple method to apply Bayes' theorem to classification problems; it is the most common of the machine learning methods to do so, especially in the area of text classification.  The algorithm is so named because it makes some "naive" assumptions about the data, namely, that all of the features in the dataset are equally important and independent - these assumptions are rarely true in most real-world applications.

For example, if you were attempting to identify spam by monitoring e-mail messages, it is almost certainly true that some features will be more important than others.  For example, the -email sender may be a more important indicator of spam than the message text.  Additionally, the words in the message body are not independent from one another, since the appearance of some words is a very good indication that other words are also likely to appear.  A message with the word _Ritalin_ will probably also contain the words _prescription_ or _drugs_.

## Classification with Naive Bayes
By adding terms, a larger likelihood table can be constructed.  Assume four words: W~1~, W~2~, W~3~, and W~4~.  As new messages arrive, a calculation of the posterior probability will determine if they are more likely to be spam or ham, given the likelihood of the words found in the message text.  For example, assume that a message contains W~1~ and W~4~ but neither W~2~ or W~3~.  Using Bayes' theorem, the following formula defines the problem:

**_P(spam $\mid$ W~1~ $\cap \neg$ W~2~ $\cap \neg$ W~3~ $\cap$ W~4~) = $\frac{P(W_1 \cap \neg W_2 \cap \neg W_3 \cap W_4 \mid spam) \times P(spam)}{P(W_1 \cap \neg W_2 \cap \neg W_3 \cap W_4)}$_**

As this does not scale well computationally, a work-around uses the fact that Naive Bayes assumes independence among events, specifically, **class-conditional independence**, which means that events are independent so long as they are conditioned on the same class value (in this case, _spam_ or _ham_).  This allows the formula to be simplified using the probability rule for independent events, which states that _P(A $\cap$ B) = P(A) $\times$ P(B)_.  Because the denominator does not depend on the class (spam or ham), it is treated as a constant value and can be ignored for the time being.  This means that the conditional probability of spam can be expressed as:

**P(spam $\mid$ W~1~ $\cap \neg$ W~2~ $\cap \neg$ W~3~ $\cap$ W~4~) $\propto$ P(W~1~ $\mid$ spam)P($\neg$ W~2~ $\mid$ spam)P($\neg$ W~3~ $\mid$ spam)P(W~4~ $\mid$ spam)P(spam)**

And the probability that the message is ham can be expressed as:

**P(ham $\mid$ W~1~ $\cap \neg$ W~2~ $\cap \neg$ W~3~ $\cap$ W~4~) $\propto$ P(W~1~ $\mid$ ham)P($\neg$ W~2~ $\mid$ ham)P($\neg$ W~3~ $\mid$ ham)P(W~4~ $\mid$ ham)P(ham)**

Note that the equals symbol has been replaced by the proportional-to symbol to indicate the fact that the denominator has been omitted.

Using some pre-canned data, the overall likelihood of spam = 0.012 while the overall likelihood of ham = 0.002. Because the division of spam/ham = 6, this particular message is 6 times more likely to be spam than ham.  However, to convert these numbers into probabilities, one last step must be performed to reintroduce the denominator that had been excluded.  Essentially, the likelihood of each outcome must be re-scaled by dividing it by the total likelihood across all possible outcomes.

In this way, the probability of spam is equal to the likelihood that the message is spam divided by the likelihood that the message is either spam or ham; the resulting value is 85.7%; if this is P(spam), then by mutual exclusivity, p(ham) = 1 = P(spam), or 14.3%.

The Naive Bayes classification algorithm used in the preceding example can be summarized by the following formula.  The probability of level _L_ for class _C_, given the evidence provided by features _F~1~_ through _F~n~_, is equal to the product of the probabilities of each piece of evidence conditioned on the class level, the prior probability of the class level, and a scaling factor _$\frac{1}{Z}$_, which converts the likelihood values into probabilities:

*P(C~L~ $\mid$ F~1~, $\ldots$, F~n~) = $\frac{1}{Z}$ p(C~L~) $\prod_{i=1}^{n}$  p(F~i~ $\mid$ C~L~)*

Although this equation seems intimidating, as the prior example illustrated, the series of steps is fairly straightforward.  Begin by building a frequency table, use this to build a likelihood table, and multiply the conditional probabilities according to the Naive Bayes' rule.  Finally, divide by the total likelihood to transform each class likelihood into a probability.







