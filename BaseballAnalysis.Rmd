---
title: "Baseball Statistics Analysis"
author: "Kevin E. D'Elia"
date: "July 11, 2016"
output: 
  html_document: 
    highlight: zenburn
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.path="./figures/")
```

## Executive Summary
What is the purpose of this analysis?  What question are you trying to answer?

## Pre-requisites/Assumptions
Before working with this dataset, it is assumed that the following pre-requisites are met:

1. The *UsingR* package has been installed using _install.packages()_ or similar method
2. The *UsingR* library has been loaded using the _library(UsingR)_ command
3. The dataset *batting* has been loaded into the working session using the _data("batting")_ command

```{r load_data, echo= FALSE}
suppressMessages(library(UsingR))
library(UsingR)
data("batting")
```

## Exploring the structure of data
```{r structure}
str(batting)
```

The data for this study is part of the UsingR package but the original source of the data is [here](http://www.seanlahman.com/baseball-archive/statistics/).
Since the data is for the year 2002 only, we can remove that column from our dataset and save it either in a new dataset, overwrite the existing dataset, or backup the original data prior to overwrite, or just overwrite and reload from the UsingR package.  We'll overwrite for now.
```{r overwrite}
batting <- batting[ , -c(2:3)]
```

### Exploring numeric variables

The five-number summary is a set of five statistics (actually, six, since R gives the mean as well!) that roughly depict the spread of a variable's values.

```{r summary_date}
summary(batting$SO)
```

### Measuring the Central Tendency - Mean and Median
Measures of Central Tendency are a class of statistics used to identify a value that falls in the middle of a set of data. 
```{r mean_median}
mean(batting$SO)
median(batting$SO)
```

What does the mean you calculate tell you about your data?  Have to form some kind of hypothesis.
The median is halfway between all the data points - what does that tell you?

How do the mean and the median compare for the variable you've chosen?
The mean and the median are affected differently by values falling at the far ends of the range.  Mean is particularly sensitive to outliers - values atypically high or low in relation to the majority of the data.  Because the mean is sensitive to outliers, it is more likely to be shifted higher or lower by a small number of extreme values.

### Measuring spread - quartiles and the five-number summary

Measuring the mean and the median provides one way to quickly summarize the values, but they don't tell much about whether or not there is diversity in the measurements.
```{r range}
range(batting$SO)
diff(range(batting$SO))
summary(batting$SO)
```

Q1 : point at which 25% of values fall below
Q3 : point at which 25% of values lie above
```{r IQR}
IQR(batting$SO)
```

```{r quantiles}
quantile(batting$SO)
quantile(batting$SO, probs = c(0.01, 0.99))
quantile(batting$SO, seq(from = 0, to = 1, by = 0.2))
```

Look at differences between min and Q1 and max and Q3.  This will help describe how the lower and upper 25% differ from the middle 50%
Also, look at the min/Q1, max/Q3 differences; which difference is greater?  The one with the greater difference will have values more spread out.
This will explain why the mean is greater than/less than the median.  It get pulled to the right for high values towards max and to the left for low values towards min.

### Visualizing numeric variables - boxplots

Displays the center and spread of a numeric variable 
```{r boxplot}
boxplot(SO ~ lgID, data = batting, col = "wheat", main = "Strikeouts by League", names = c("American\nLeague", "National\nLeague"), ylab = "Strikeouts", las = 1, border = "grey")
```

The box-and-whiskers plot depicts the five-number summary values . min, q1/q2 (median)/q3, max
The whiskers can extend to a min or max of 1.5 * IQR below Q1 or above Q3; any values beyond this threshold are considered outliers.
Explain the outliers you see in your boxplots, specifically, how their presence or absence affects the mean calculation.

### Visualizing numeric variables - histograms

Create a histogram of strikeouts
```{r histogram}
hist(batting$SO, col = "light blue", border = "dark blue", main = "Strikeout Frequencies", xlab = "Strikeouts (2002)", ylab = "Player Count", las = 1)
```

describe the histogram results
The player count is the number of players that struck out within each of the ranges (0-20, etc.)  Knowing the total number of players gives percentages, i.e., x percent of all players had strikeouts within range Y.  also, talk about the skew of the histogram - this one is right-skewed, meaning it has outliers to the right of the data.  values on the high end are far more spread out than the values on the low end left side.

### Understanding numeric data - uniform and normal distributions

A variable's distribution describes how likely a value is to fall within various ranges.  Is the data you are working with uniform or normal or neither?

### Measuring spread - variance and standard deviation

Distributions allow us to characterize a large number of values using a smaller number of parameters.  The normal distribution can be defined with just two:  center and spread.  Center = mean, spread = standard deviation.

$\sigma^2 = {\frac{1}{N}\sum\limits_{i = 1}^N {\left( {x_i - \bar x} \right)^2 } }$

$\sigma = \sqrt {\frac{1}{N}\sum\limits_{i = 1}^N {\left( {x_i - \bar x} \right)^2 } }$

While interpreting variance, larger numbers indicate that the data are spread more widely around the mean.  The standard deviation indicates, on average, how much each value differs from the mean.
```{r variance}
var(batting$SO)
sd(batting$SO)
```
The standard deviation can be used to quickly estimate how extreme a given value is under the assumption that it came from a normal distribution.  Use the *68-95-99.7* rule (1, 2, 3 sd from the mean).  So, 68% of the strikeouts were blah, blah but this is not a normal distribution.

```{r qqplot}
qqnorm(batting$SO)
qqline(batting$SO)
```

## Exploring categorical variables

In contrast to numeric variables, categorical variables are typically examined using tables rather than summary statistics.  A table that presents a single categorical variable is called a *one-way table*.  The _table()_ function is used for this purpose.
```{r table}
table(batting$teamID)
```

The _table()_ output lists the categories of nominal variables and a count of the number of values in each category.  Since we know that there are `r nrow(batting)` teams, we can estimate percentages of players per team.  Or, the _prop.table()_ function can be used for this purpose:
```{r proportions}
prop.table(table(batting$teamID))
```

Clearly, this are hard-to-read results.  This approach produces a more pleasing output:
```{r rounded}
teams <- table(batting$teamID)
teams.pct <- prop.table(teams) * 100
round(teams.pct, 1)
```

We can then figure out which teams are most/least represented in this data.

### Measuring the central tendency - the mode

In statistics terms, the mode of a feature is the value occurring most often.  There is *unimodal*, *bimodal*, and *multimodal* data.  The _table()_ function is used to calculate the mode for categorical variables.  The *mean* and the *median* are not defined for nominal variables.

What is the mode for this dataset?

The mode or modes are used in a qualitative sense to gain an understanding of important values.  The common value is not necessarily a majority.  Black car might be most common for the color variable but they were only a quarter of the available data.  Think about modes in relation to the other categories.  Is there one or several which dominate?  What does that tell us about the variable being measured.

## Exploring relationships between variables

Calculations so far have been for *univariate* data.  There are also *bivariate* and *multivariate* data.  To do that, scatterplots are used.

### Visualizing relationships - scatterplots

A *scatterplot* is a digaram that visualizes _bivariate_ relationships.
```{r scatterplots}
with(batting, plot(x = teamID, y = SO))
with(batting, plot(x = AB, y = SO))
```

Can the categorical scatterplot be improved? What relationship does the plot show?  Are there outliers that indicate some unusual indications, like high at-bats and low strikeouts (more than what's in the trend).  This is a positive trend, while there are negative trends.  A flat line or seemingly random scattering of dots, is evidence that the two variables are not associated at all.  The strength of a linear association between two variables is measured by correlation.
```{r correlation}
with(batting, round(cor(AB, SO), 2))
```

### Examining relationships - two-way cross-tabluations

To examine a relationship between two nominal variables, *two-way cross-tabulation* (also known as a *crosstab* or *contingency table*).  A cross-tabulation is similar to a scatterplot in that it allows you to examine how the values of one variable vary by the values of another.  The format is a table in which the rows are the levels of one variable, while the columns are the levels of another.  Counts in each of the table's cells indicate the number of values falling into the particular row and column combination.

Some times the number of levels in one variable need to be reduced.  In the case of the batting data, there are `r length(levels(batting$teamID))` levels!
You have to identify some way of grouping the variables.  This can be done using a binary indicator variable (often called a *dummy variable*), indicating which of two groups members of the category fall into.  In this case, the levels will be split into two groups of 15 levels each.

```{r gmodels}
library(gmodels)
j <- levels(batting$teamID)
batting$lower <- batting$teamID %in% j[1:15]
with(batting, CrossTable(x = lower, y = lgID))
```

## Understanding nearest neighbor classification

In a single sentence, *nearest neighbor* classifiers are defined by their characteristic of classifying unlabeled examples by assigning them the class of similar labeled examples.  In general, nearest neighbor classifiers are well-suited for classification tasks where relationships among the features and the target classes are numerous, complicated, or extremely difficult to understand, yet the items of similar class type tend to be fairly homogeneous.  If a concept is difficult to define, but you know it when you see it, k-NN might be appropriate.  but, if the data is noisy and thus no clear distinctino exists among the groups, the k-NN algorithm may struggle.

### k-NN algorithm

#### Strengths

* simple and effective
* makes no assumptions about the underlying data distributino
fast training phase

#### Weaknesses

* does not produce a model, limiting the ability to understand how the features are related to the class
* requires selection of an appropriate _k_
* slow classificatino phase
* nominal features and missing data require additional processing

The k-NN algorithm gets its name from the fact that it uses information about an example's k-nearest neighbors to classify unlabeled examples.  The letter _k_ is a variable term implying that any number of nearest neighbors could be used.  After choosing _k_, the algorithm requires a training dataset made up of examples that have been classified into several categories, as labeled by a nominal variable.  Then, for each unlabeled (i.e., unclassified) record in the test dataset, k-NN identifies _k_ records in the training data that are the "nearest" in similarity.  The unlabeled test instance is assigned the class of the majority of the k nearest neighbors.

A scatterplot could be used for two-dimensional data, such as x = how sweet and y = how crunchy.  The scatterplot may show groupings of the data, especially if plotted with label names (so all the protein-based foods appear in one cluster, etc.).

### Measuring similarity with distance

Locating the tomato's nearest neighbors requires a *distance function*, or a formula that measures the similarity between the two instances.

Traditionally, the k-NN algorithm uses *Euclidean distance*, which is the distance one would measure if it were possible to use a ruler to connect two points.  The _dist()_ function in *R* is helpful for this (q.v., ?dist)

dist(_p,q_) = $\sqrt(p_1 - q_1) ^ 2 + (p_2 - q_2) + ... + (p_n - q_n)$

You calcluate the distance between the tomato and the green bean.  The tomato has a crunchiness and a sweetness rating, those are the _p_ values, while the green bean has its own values; those are the _q_ values.  Plug them in for green bean and other examples to produce a column in the matrix with the results of the distance measurements.  when you are done, you classify the tomato based on the food type of its single nearest neighbor.  This is called *1-NN classification* because k=1.  If you use k=3, it will take a vote from amongst the 3 nearest neighbors to arrive at a classification.

### Choosing an appropriate k
The decision of how many neighbors to use for k-NN determines how well the model will generalize to future data.  The balance between overfitting and underfitting the training data is a problem known as *bias-variance tradeoff*.  Choosing a large _k_ reduces the inmpact of variance caused by nooisy data, but can bias the learner so that it runs the risk of ignoring small but important patterns.

In practice, choosing _k_ depends on the difficulty of the concept to be learned, and the number of records in the training data.  One common practice is to begin with _k_ equal to the square root of the number of training examples.

An alternative approach is to test several _k_ values on a variety of test datasets and choose the one that delivers the best classification performance.

### Preparing data for use with k-NN

Features are typically transformed to a standard range prior to applying the k-NN algorithm.  This is because the distance formula is highly dependent on how features are measured.  If certain features have a much larger range of values than the others, the distance measurements will be strongly dominated by the features with the larger ranges.  So, if we add the Scoville scale (a standard spiciness measurement) as one of the features, its range is 0-$10^6$ and so it's affect would overwhelm the effects of the other features.

The solution is to rescale the features by shrinking or expanding their range such tat each one contributes relatively equally to the distance formula.  So, we would make the spiciness range go from 1 to 10 in this case.

The traditional method of rescaling features for k-NN is *min-max normalization*.  This process transforms a feature such that all of its values fall in a range between - and 1.  The formula for normalizing a feature is:

$X_new$ = X - min(X)/ max(X) - min(X)

Normalized feature values can be interpreted as indicating how far, from 0 percent to 100 percent, the original value fell along the range between the original minimum and maximum.

Another common transofrmation is called *z-score standardization*.  

$X_new = X - \mu / \sigma$ = X - Mean(X)/StdDev(X)

This rescales each of the feature's values in terms of how many standard deviatinos they fall above or below the mean value.  The resulting value is called a *z-score*.  The z-scores fall in an unbounded range of negative and positive numbers.  Unlike normalized values, they have no predefined minimum and maximum.

*Note:* The same rescaling method used on the k-NN training dataset must also be applied to the examples the algorithm will later classify.  The min-max of examples may have values outside the range observed in training data; find reasonable constants for the formula rather than the observed values.  Alternatively, use the z-score standardization

The Euclidean distance formula is not defined for nominal data.  Therefore, to calculate the distance between nominal features, we need to convert them into a numeric format utilizing *dummy coding*, where a value of 1 indicates one category and 0 another.  An _n_-category nominal feature can be dummy coded by creating the binary indicator variable for (_n - 1_) levels of the feature.  For example, the dummy coding for a three-category temperature variable (hot, medium, cold) could be set up as 2 features (hot = 1/0, medium = 1/0, cold would be both of these = 0, no need for separate cold dummy variable)

### Why is the k-NN algorithm lazy?

Classification algorithms based on the nearest neighbor methods are considered *lazy learning* algorithms because, technically speaking, no abstraction occurs.  The abstraction and generalization processes are skipped altogether.  A lazy learner is not really learning anything; instead, it merely stores the training data verbatim. Lazy learning is also known as *instance-based learning* or *rote learning*. 

As instance-based learners do not build a model, the methid is said to be in a class of *non-paramterc* learning methods - no parameters are learned about the data.  Without generating theories about the underlying data, non-parametric methods limit our ability to understand how the classifier is using the data.  On the other hadn, this allows the learner to find natural patterns rather than trying to fit the data into a preconceived and potentially biased functional form.

## Example - diagnosing breast cancer with the k-NN algorithm

We will investigate the utility of machine learning for detecing cancer by applying the k-NN algorithm to measurements of biopsied cells from woemn with abnormal breast masses.

### Step 1 - collecting data

```{r data_collection}
#URL <- "http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
#download.file(URL, destfile = "./data/wdbc.data.csv", method="curl")
```

5. Number of instances: 569 

6. Number of attributes: 32 (ID, diagnosis, 30 real-valued input features)

7. Attribute information

1) ID number
2) Diagnosis (M = malignant, B = benign)
3-32)

Ten real-valued features are computed for each cell nucleus:

	a) radius (mean of distances from center to points on the perimeter)
	b) texture (standard deviation of gray-scale values)
	c) perimeter
	d) area
	e) smoothness (local variation in radius lengths)
	f) compactness (perimeter^2 / area - 1.0)
	g) concavity (severity of concave portions of the contour)
	h) concave points (number of concave portions of the contour)
	i) symmetry 
	j) fractal dimension ("coastline approximation" - 1)

### Step 2 - exploring and preparing the data

```{r readin_data}
# For Mac
# setwd("/Users/kevindelia/Development/MachineLearningWithR/data")
# Fo HP
setwd("/home/kevin/R Work/Machine Learning with R, Second Edition_Code/Chapter 03")
# may need to provide the data and attribute it to the author
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)

# examine the structure of the wbcd data frame
str(wbcd)

# drop the id feature
wbcd <- wbcd[-1]
```

Regardless of the machine learning method, ID variables should always be excluded.  Neglecting to do so can lead to erroneous findings beccause the ID can be used to uniquely "predict" each example.  Therefore, a model that includes an identifier will suffer from overfitting, and is unlikely to generalize well to other data.

Diagnosis is of particular interest as it is the outcome that is to be predicted.
```{r diagnosis_table}
# table of diagnosis
table(wbcd$diagnosis)
```

Many R machine learning classifiers require that the target feature is coded as a factor.  Give the diagnosis value more informative labels:
```{r relabel}
# recode diagnosis as a factor
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))
```

Now use _prop.table()_ to get percentages:

```{r percent_diagnosis}
# table or proportions with more informative labels
round(prop.table(table(wbcd$diagnosis)) * 100, digits = 1)
```

The remaining 30 features are all numeric and are different measurements of ten characteristics.  Taking a closer look at three of these features:
```{r feature_summary}
# summarize three numeric features
summary(wbcd[c("radius_mean", "area_mean", "smoothness_mean")])
```

Since the range of *smoothness* is much different than that of *area*, the impact of area is going to be much larger than the smoothness in the distance calculation.  Normalization will rescale the features to a standard range of values to avoid problems for the classifier.

#### Transformation - normalizing numeric data

Create a normalize function that matches the formula given above:
```{r normalize}
# create normalization function
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```

Now use _lapply()_ to work on our data
```{r standardize}
# normalize the wbcd data
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normalize))

# confirm that normalization worked
summary(wbcd_n$area_mean)
```

#### Data preparation - creating training and test datasets

Although all 569 biopsies are labeled with a benign or malignant status, it is not very interesting to predict what we already know.  Additionally, any performance measures we obtain during the training may be misleading as we do not know the extent to which cases have been overfitted or how well the learner will generalize to unseen cases.  A more interesting question is how well our learner performs on a dataset of unlabeled data.

In the absence of such data, we can simulate this scencario by dividing our data into two portions:  a training dataset that will be used to build the k-NN model and a test dataset that will be used to estimate the predictive accuracy of the model.
```{r splitdata}
# create training and test data
wbcd_train <- wbcd_n[1:469, ]
wbcd_test <- wbcd_n[470:569, ]
```

The nornalized dataset excluded column 1 *diagnosis*.  For training the k-NN model, we will need to store these class labels in factor vectors.  The following code takes the *diagnosis* factor in the first column of the _wbcd_ data frame, and this data will be used in the next steps of training and evaluating the classifiers.

```{r labeldata}
# create labels for training and test data
wbcd_train_labels <- wbcd[1:469, 1]
wbcd_test_labels <- wbcd[470:569, 1]
```

### Step 3 - training a model on the data

For the k-NN algorithm, the training phase actually involves no model building; the process of training a lazy learner like k-NN simply involves storing the input data in a structured format.

The k-NN implemention used is from the *class* package, which should be one of the pre-requisite installations prior to performing this analysis (note it in the appropriate section).
```{r class_install}
# load the "class" library
library(class)
```

The _knn()_ function in the *class* package provides a standard, classic implementation of the k-NN algorithm.  For each instance in the test data , the function will identify the k-Nearest neighbors, using Euclidean distance, where _k_ is a user-specified number.  The test instance is classified by taking a "vote" among the --Nearest Neighbors - specifically, this involves assigning the class of the amority of the _k_ neighbors.  A tie vote is broken at random.

*Note:*  There are more sophisticated implementations; search CRAN for k-NN

#### Using the _knn()_ function in the *class* package:
p <- knn(train, test, class, k)
where:

* train is a data frame containing numeric training data

* test is a data frame containing numeric test data

* class is a factor vector with the class for each row in the training data

* k is an integer indicating the number of nearest neighbors

The function returns a factor vector of predicted classes for each row in the test data frame.

The square root of `r nrow(wbcd_train)` is `r round(sqrt(nrow(wbcd_train)), 1)`, so 21 is a good estimate for our value of _k_.
```{r knn}
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
```

The _knn()_ function returns a factor vector of predicted labels for each of the examples in the *test* dataset, which is contained in the *wbcd_test_pred* variable.

### Step 4 - evaluating model performance
The next step of the process is to evaluate how well the predicted classes in the *wbcd_test_pred* vector match up with the known values in the *wbcd_test_labels* vector.  Use the _CrossTable()_ function from the *gmodels* package.  specifying *prop.chisq = FALSE* removes the unneccessary chi-square values from the output.
```{r xtabs}
# Create the cross tabulation of predicted vs. actual
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
```

The cell percentages in the table indicate the proportion of values that fall into four categories.  The top-left cell indicates the *tre negative* results.  These 61 of the 100 values are cases where the mass was benigh and the k-NN algorithm correctly identified it as such.  The bootom-right cell indicates the *true positive* results, where the classifier and the clinically determinded label agree that the mass is malignant.  A total of 37 of 100 predictions were true positives.

The celles falling on the other diagonal contain counts of examples where the k-NN approach disagreed with the true lable.  The two examples in the lower_left cell are *false negative* results; in this case, the predicted value was benigh, but the tumor was actually malignant.  errors in this direction could be extremely costly as they might lead a patient to believe that she is cancer-free, but in reality, the disease may continue to spread.  The top-right cell would conatin the *false positive* results, if there were any.  These values occur when the model classifies a mass as malignant but in reality, ti was benigh.  Although such errors are less dangerous than a flse negative result, they should also be avoided as they could lead to additiona financial burden on the health care system or additional stress for the patient as additional tests or treatmen may have gol  be provided.

A total of 2 out of 100, or 2% of masses were incorrectly classified by the k-NN approach.  can the erroros that are dangerous false negatives be reduced?

### Step 5 - improving model performance

The two approachs to try are:

1. employ an alternative method for rescaling the numeric features

2. try several different values of _k_

#### Transformation - z-score standardization

Although normalizatino is traditionally used for k-NN classification, it may not always be the most appropriate way to rescale features.  Since the z-score standardized values have no predifeind minimum and maximum, extreme values are not compressed towards the center.  One might suspect that with a malignant tumor, we might see some very extremem outliers as the tumors groun uncontrollably.  It might, therefore, be reasonable to allow the outliers to be weighted more heavily in the distance calculation.

To standardize a vectory, use the built-in _scale()_ function
```{r scale}
# use the scale() function to z-score standardize a data frame
wbcd_z <- as.data.frame(scale(wbcd[-1]))
```

This command rescales all the features, with the exception of *diagnosis* and stores the result in the _z dataframe (for z-score transformed)

Now look at some summary statistics to confirm the transformation
```{r transform_summary}
# confirm that the transformation was applied correctly
summary(wbcd_z$area_mean)
```

The mean of a z-score standardized variable should always be zero, and the range should be fairly compact.  A z-score greater than 3 or less than -3 indicates an extremely rare value.  The transormation appears to have worked.  Now, perform the same series of steps from the previous effort, using the standardized data this time.
```{r redo}
# create training and test datasets
wbcd_train <- wbcd_z[1:469, ]
wbcd_test <- wbcd_z[470:569, ]

# re-classify test cases
wbcd_test_pred <- knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)

# Create the cross tabulation of predicted vs. actual
CrossTable(x = wbcd_test_labels, y = wbcd_test_pred, prop.chisq = FALSE)
```

Unfortunately, the results of the new transformation show a slight decline in accuracy.  The instances which were correctly classified 98 percent of examples previously, only 95 percent were classified correctly this time.  Making mattters worse, this approach did no better at classifyin the dangerous false negatives.
